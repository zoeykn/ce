{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Homework 2**: Working with LLMs via API\n",
    "\n",
    "### ðŸ“… **Due Date**: Day of Lecture 3, 11:59 PM\n",
    "\n",
    "#### ðŸ”— **My Repository**: https://github.com/YOUR-USERNAME/ai-engineering-fordham\n",
    "\n",
    "*(Replace the URL above with your actual repository URL)*\n",
    "\n",
    "**Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Movie Poster Generator\n",
    "\n",
    "In this homework, you'll build a mini-application that:\n",
    "1. **Extracts** structured movie data from text descriptions using Pydantic\n",
    "2. **Processes** multiple movies concurrently using async programming\n",
    "3. **Explores** temperature, logprobs, and reasoning models\n",
    "4. **Generates** movie posters using AI image generation\n",
    "\n",
    "This project combines key skills from Lecture 2: structured outputs, async programming, LLM parameters, and image generation.\n",
    "\n",
    "**Total Points: 145** (+ 10 bonus)\n",
    "\n",
    "---\n",
    "\n",
    "### A Note on Using Resources\n",
    "\n",
    "You are encouraged to use any resources to complete this homework:\n",
    "- **ChatGPT / Claude** - Ask AI to explain concepts or help debug\n",
    "- **Lecture 2 notebook** - Reference the examples we covered\n",
    "- **Official documentation** - LiteLLM, Pydantic, Google GenAI docs\n",
    "\n",
    "When you use external resources, please cite them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Environment Setup (10 points)\n",
    "\n",
    "First, let's verify your environment is set up correctly.\n",
    "\n",
    "### 1a. Verify imports work (5 pts)\n",
    "\n",
    "Run the cell below. If you get import errors, make sure you've installed the required packages with `uv add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1a: Verify imports work (5 pts)\n",
    "import litellm\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Verify API keys are set (5 pts)\n",
    "\n",
    "Test that your API keys work by making a simple call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1b: Verify API keys (5 pts)\n",
    "# Make a simple test call to verify your OpenAI API key works\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'API working!' and nothing else.\"}],\n",
    "    max_tokens=20\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Design the Movie Schema (15 points)\n",
    "\n",
    "Design a Pydantic model to represent movie data. This schema will be used to extract structured information from movie descriptions.\n",
    "\n",
    "**Requirements:**\n",
    "- `title` - string, required\n",
    "- `genre` - use `Literal` with at least 4 genre options (e.g., \"sci-fi\", \"drama\", \"action\", \"comedy\", etc.)\n",
    "- `year` - integer with validation (must be between 1900 and 2030)\n",
    "- `main_characters` - list of strings (1-5 characters)\n",
    "- `mood` - string describing the emotional tone\n",
    "- `visual_style` - string describing how the movie looks visually\n",
    "- `tagline` - optional string (the movie's catchphrase)\n",
    "\n",
    "**Hints:**\n",
    "- Use `Field(ge=..., le=...)` for numeric validation\n",
    "- Use `Field(min_length=..., max_length=...)` for list length validation\n",
    "- Use `| None = None` for optional fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Design your Movie schema (15 pts)\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"Structured representation of a movie.\"\"\"\n",
    "    # YOUR CODE HERE - design the schema based on the requirements above\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your schema by creating a Movie object\n",
    "# This should work if your schema is correct\n",
    "\n",
    "test_movie = Movie(\n",
    "    title=\"The Matrix\",\n",
    "    # YOUR CODE HERE - fill in the rest of the fields\n",
    ")\n",
    "\n",
    "print(test_movie.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Extract Movie Data with Structured Outputs (20 points)\n",
    "\n",
    "Write a function that takes a movie description and uses LiteLLM with structured outputs to extract a `Movie` object.\n",
    "\n",
    "**Hints:**\n",
    "- Use `litellm.completion()` with `response_format=Movie`\n",
    "- The LLM will automatically return data matching your schema\n",
    "- Parse the JSON response into a Movie object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Write a function to extract movie data (20 pts)\n",
    "\n",
    "def extract_movie(description: str) -> Movie:\n",
    "    \"\"\"\n",
    "    Use LiteLLM with structured outputs to extract movie data.\n",
    "    \n",
    "    Args:\n",
    "        description: A text description of a movie\n",
    "        \n",
    "    Returns:\n",
    "        A Movie object with the extracted data\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function with this description (Avatar)\n",
    "\n",
    "test_description = \"\"\"\n",
    "The year is 2154. Jake Sully, a paralyzed marine, is sent to the moon Pandora \n",
    "where he falls in love with a native Na'vi woman named Neytiri while on a mission \n",
    "to infiltrate their tribe. The film is a visually stunning sci-fi epic with \n",
    "bioluminescent forests and floating mountains. It explores themes of \n",
    "environmentalism and colonialism with an awe-inspiring, hopeful tone.\n",
    "\"\"\"\n",
    "\n",
    "movie = extract_movie(test_description)\n",
    "print(movie.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Async Batch Processing (20 points)\n",
    "\n",
    "Now let's process multiple movies concurrently! This is much faster than processing them one at a time.\n",
    "\n",
    "### 4a. Write an async version of extract_movie (10 pts)\n",
    "\n",
    "**Hints:**\n",
    "- Use `async def` instead of `def`\n",
    "- Use `await litellm.acompletion()` instead of `litellm.completion()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4a: Write an async version of extract_movie (10 pts)\n",
    "\n",
    "async def async_extract_movie(description: str) -> Movie:\n",
    "    \"\"\"Extract movie data asynchronously.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Process all descriptions concurrently (10 pts)\n",
    "\n",
    "**Hints:**\n",
    "- Create a list of tasks using list comprehension\n",
    "- Use `asyncio.gather(*tasks)` to run them all concurrently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are 5 movie descriptions to process:\n",
    "movie_descriptions = [\n",
    "    \"\"\"A dinosaur theme park on a remote island goes terribly wrong when the security \n",
    "    systems fail during a tropical storm. Scientists and visitors must survive against \n",
    "    escaped prehistoric predators. Directed with Spielberg's signature sense of wonder \n",
    "    and terror, featuring groundbreaking CGI dinosaurs.\"\"\",\n",
    "    \n",
    "    \"\"\"A young boy discovers on his 11th birthday that he's actually a famous wizard \n",
    "    in the magical world. He attends a school for witchcraft where he makes friends, \n",
    "    learns magic, and uncovers the mystery of his parents' death. A whimsical fantasy \n",
    "    with gothic British atmosphere.\"\"\",\n",
    "    \n",
    "    \"\"\"In a world where skilled thieves can enter people's dreams to steal secrets, \n",
    "    one man is offered a chance to have his criminal record erased if he can do the \n",
    "    impossible: plant an idea in someone's mind. A mind-bending thriller with \n",
    "    rotating hallways and cities folding on themselves.\"\"\",\n",
    "    \n",
    "    \"\"\"A young lion prince is tricked by his uncle into thinking he caused his \n",
    "    father's death and flees into exile. Years later, he must return to reclaim \n",
    "    his kingdom. An animated musical epic set on the African savanna with \n",
    "    stunning hand-drawn animation.\"\"\",\n",
    "    \n",
    "    \"\"\"In a dystopian future where Earth is dying, a team of astronauts travels \n",
    "    through a wormhole near Saturn to find a new home for humanity. A father \n",
    "    must choose between seeing his children again and saving the human race. \n",
    "    Epic space visuals with an emotional core.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4b: Process all descriptions concurrently (10 pts)\n",
    "\n",
    "async def extract_all_movies(descriptions: list[str]) -> list[Movie]:\n",
    "    \"\"\"Process all movie descriptions concurrently and return results.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and time it!\n",
    "start = time.time()\n",
    "movies = await extract_all_movies(movie_descriptions)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Processed {len(movies)} movies in {elapsed:.2f} seconds\")\n",
    "print()\n",
    "for m in movies:\n",
    "    print(f\"  - {m.title} ({m.year}) - {m.genre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Understanding Temperature (15 points)\n",
    "\n",
    "Temperature controls how \"random\" or \"creative\" an LLM's outputs are:\n",
    "\n",
    "| Temperature | Behavior |\n",
    "|-------------|----------|\n",
    "| **0.0** | Deterministic - always picks the most likely token |\n",
    "| **0.7** | Balanced - some creativity while staying coherent |\n",
    "| **1.0** | Default - moderate randomness |\n",
    "| **1.5+** | High creativity - more surprising/diverse outputs |\n",
    "\n",
    "### 5a. Temperature Comparison (10 pts)\n",
    "\n",
    "Run the same creative prompt at different temperatures (0.0, 0.7, 1.0, 1.5) **three times each**. Observe:\n",
    "- At temperature 0, do you get the same output every time?\n",
    "- How does creativity/variety change as temperature increases?\n",
    "\n",
    "**Hints:**\n",
    "- Use `temperature=X` parameter in `litellm.completion()`\n",
    "- Use the provided prompt about movie taglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5a: Temperature Comparison (10 pts)\n",
    "\n",
    "# Use this creative prompt for testing\n",
    "creative_prompt = \"Write a one-sentence movie tagline for a sci-fi thriller about AI.\"\n",
    "\n",
    "temperatures = [0.0, 0.7, 1.0, 1.5]\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# For each temperature, call the LLM 3 times and print the results\n",
    "# Observe: Are outputs at temperature 0 identical? How do higher temperatures differ?\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    for i in range(3):\n",
    "        # YOUR CODE HERE - make a completion call with the temperature parameter\n",
    "        # response = litellm.completion(...)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Analyze Output Diversity (5 pts)\n",
    "\n",
    "Write a function that generates N completions at a given temperature and measures how diverse the outputs are.\n",
    "\n",
    "**Hints:**\n",
    "- Generate multiple completions and count unique outputs\n",
    "- A simple diversity metric: `unique_outputs / total_outputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5b: Analyze Output Diversity (5 pts)\n",
    "\n",
    "def measure_diversity(prompt: str, temperature: float, n_samples: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Generate n_samples completions and measure diversity.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the LLM\n",
    "        temperature: Temperature setting (0.0 to 2.0)\n",
    "        n_samples: Number of completions to generate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'outputs' (list), 'unique_count' (int), 'diversity_ratio' (float)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your diversity function\n",
    "test_prompt = \"Name a color.\"\n",
    "\n",
    "print(\"Testing diversity at different temperatures:\\n\")\n",
    "for temp in [0.0, 1.0, 1.5]:\n",
    "    result = measure_diversity(test_prompt, temperature=temp, n_samples=5)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  Outputs: {result['outputs']}\")\n",
    "    print(f\"  Unique: {result['unique_count']}/{5}\")\n",
    "    print(f\"  Diversity ratio: {result['diversity_ratio']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Understanding Logprobs (15 points)\n",
    "\n",
    "**Logprobs** (log probabilities) let you see \"inside\" the model's decision-making. For each token generated, you can see:\n",
    "- The probability the model assigned to the chosen token\n",
    "- Alternative tokens the model considered (and their probabilities)\n",
    "\n",
    "This helps you understand:\n",
    "- How \"confident\" the model is in its outputs\n",
    "- What other options it was considering\n",
    "- Why certain generations might be more reliable than others\n",
    "\n",
    "### 6a. Request and View Logprobs (10 pts)\n",
    "\n",
    "Make a completion request with `logprobs=True` and `top_logprobs=5` to see the top 5 token alternatives for each position.\n",
    "\n",
    "**Hints:**\n",
    "- Add `logprobs=True` and `top_logprobs=5` to your completion call\n",
    "- Access logprobs via `response.choices[0].logprobs.content`\n",
    "- Each token has a `top_logprobs` list with alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6a: Request and View Logprobs (10 pts)\n",
    "\n",
    "# Make a completion request with logprobs enabled\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"The capital of France is\"}],\n",
    "    max_tokens=10,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5  # Get top 5 alternatives for each token\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Print the generated text\n",
    "# 2. Access response.choices[0].logprobs.content\n",
    "# 3. For each token, print the token and its top 5 alternatives with probabilities\n",
    "\n",
    "# Hint: logprobs are in log scale. To convert to probability: prob = exp(logprob)\n",
    "import math\n",
    "\n",
    "print(\"Generated text:\", response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Token-by-token analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# YOUR CODE HERE - iterate through logprobs and display alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Visualize Token Probabilities (5 pts)\n",
    "\n",
    "Create a simple visualization showing the probability distribution for a specific token position. You can use a bar chart or ASCII art.\n",
    "\n",
    "**Hints:**\n",
    "- Pick an interesting token position (e.g., where the model had to make a choice)\n",
    "- Convert logprobs to probabilities using `math.exp(logprob)`\n",
    "- A simple bar chart: `\"â–ˆ\" * int(prob * 50)` gives you ASCII bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6b: Visualize Token Probabilities (5 pts)\n",
    "\n",
    "def visualize_token_probs(logprobs_content, token_index: int = 0):\n",
    "    \"\"\"\n",
    "    Visualize the probability distribution for a specific token position.\n",
    "    \n",
    "    Args:\n",
    "        logprobs_content: The logprobs.content from the response\n",
    "        token_index: Which token position to visualize (0 = first token)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    # 1. Get the top_logprobs for the specified token_index\n",
    "    # 2. Convert logprobs to probabilities\n",
    "    # 3. Create a visualization (bar chart or ASCII art)\n",
    "    pass\n",
    "\n",
    "# Test your visualization on the first token\n",
    "# visualize_token_probs(response.choices[0].logprobs.content, token_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 7: Reasoning Models (15 points)\n",
    "\n",
    "**Reasoning models** like OpenAI's o3-mini are designed to \"think through\" complex problems before answering. They:\n",
    "- Break down problems into steps\n",
    "- Consider multiple approaches\n",
    "- Show their reasoning process\n",
    "- Excel at logic puzzles, math, and code\n",
    "\n",
    "### 7a. Using o3-mini for Complex Reasoning (10 pts)\n",
    "\n",
    "Use OpenAI's o3-mini reasoning model through LiteLLM to solve a complex logic puzzle.\n",
    "\n",
    "**Hints:**\n",
    "- Use `model=\"o3-mini\"` in your litellm call\n",
    "- Reasoning models work best with challenging problems\n",
    "- Observe how the response shows step-by-step thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7a: Using o3-mini for Complex Reasoning (10 pts)\n",
    "\n",
    "# A challenging logic puzzle\n",
    "logic_puzzle = \"\"\"\n",
    "Three friends (Alice, Bob, and Carol) each have a different pet (cat, dog, fish) \n",
    "and a different favorite color (red, blue, green).\n",
    "\n",
    "Clues:\n",
    "1. Alice doesn't have the cat.\n",
    "2. The person with the dog likes blue.\n",
    "3. Carol likes green.\n",
    "4. Bob doesn't have the fish.\n",
    "\n",
    "Who has which pet and what is their favorite color?\n",
    "Solve this step by step.\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# Use o3-mini to solve this logic puzzle\n",
    "# response = litellm.completion(\n",
    "#     model=\"o3-mini\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": logic_puzzle}]\n",
    "# )\n",
    "\n",
    "# Print the response and observe the reasoning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Compare Reasoning vs Non-Reasoning (5 pts)\n",
    "\n",
    "Now solve the same puzzle using `gpt-5-mini` (a non-reasoning model) and compare the results.\n",
    "\n",
    "**Questions to consider:**\n",
    "- Does the non-reasoning model show step-by-step thinking?\n",
    "- Which model gets the correct answer?\n",
    "- How does the response structure differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7b: Compare Reasoning vs Non-Reasoning (5 pts)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Send the same logic_puzzle to gpt-5-mini\n",
    "# 2. Compare the response to o3-mini's response\n",
    "\n",
    "# response_standard = litellm.completion(\n",
    "#     model=\"gpt-5-mini\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": logic_puzzle}]\n",
    "# )\n",
    "\n",
    "# Print and compare:\n",
    "# - Did both models get the correct answer?\n",
    "# - How did their reasoning processes differ?\n",
    "# - Which response was more helpful/clear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 8: Generate Movie Poster (20 points)\n",
    "\n",
    "Now for the fun part - generating movie posters using AI!\n",
    "\n",
    "### 8a. Design a prompt generator (5 pts)\n",
    "\n",
    "Write a function that takes a `Movie` object and creates a detailed image generation prompt.\n",
    "\n",
    "**Your prompt should incorporate:**\n",
    "- The movie's visual style\n",
    "- The mood/tone\n",
    "- Key visual elements that represent the genre\n",
    "- Professional movie poster composition\n",
    "\n",
    "**Tip:** Aim for 50-100 words. Be specific about colors, composition, and style!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8a: Design your prompt generator (5 pts)\n",
    "\n",
    "def generate_poster_prompt(movie: Movie) -> str:\n",
    "    \"\"\"\n",
    "    Create a detailed image generation prompt from movie data.\n",
    "    \n",
    "    Returns a detailed prompt string (aim for 50-100 words)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE - design your prompt template\n",
    "    # Consider: How can you use the movie's mood, visual_style, and genre\n",
    "    # to create an evocative image prompt?\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your prompt generator\n",
    "chosen_movie = movies[0]  # or pick your favorite from the list!\n",
    "prompt = generate_poster_prompt(chosen_movie)\n",
    "\n",
    "print(f\"Prompt for '{chosen_movie.title}':\")\n",
    "print()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Generate the actual image (10 pts)\n",
    "\n",
    "Use Google's Gemini to generate the movie poster.\n",
    "\n",
    "**Hints:**\n",
    "- Use `genai.Client()` to create a client\n",
    "- Use `client.models.generate_content()` with `model=\"gemini-2.5-flash-image\"`\n",
    "- The response will have an image in `response.candidates[0].content.parts`\n",
    "- Save the image to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8b: Generate the movie poster (10 pts)\n",
    "\n",
    "# Create Google client\n",
    "google_client = genai.Client()\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# 1. Generate the image using gemini-2.5-flash-image\n",
    "# 2. Extract the image from the response\n",
    "# 3. Save it to temp/poster_{movie_title}.png\n",
    "#    (Create the temp directory if it doesn't exist)\n",
    "\n",
    "# Make sure to create temp directory\n",
    "os.makedirs(\"temp\", exist_ok=True)\n",
    "\n",
    "# Generate and save your poster here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Display the image (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8c: Display the saved image (5 pts)\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# YOUR CODE HERE - display the poster you saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 9: Submit via Pull Request (15 points)\n",
    "\n",
    "Now let's practice a real-world development workflow! Instead of pushing directly to `main`, you'll create a **branch**, open a **Pull Request (PR)**, and **merge** it.\n",
    "\n",
    "This is how professional developers submit code for review. Your TA will check your merged PR to verify your submission.\n",
    "\n",
    "### 9a. Create a new branch (5 pts)\n",
    "\n",
    "Run this command in your terminal to create and switch to a new branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9a: Create a new branch (5 pts)\n",
    "# Run this in your terminal (not in this notebook!)\n",
    "\n",
    "!git checkout -b homework-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Commit your work (5 pts)\n",
    "\n",
    "Stage all your changes and create a commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9b: Commit your work (5 pts)\n",
    "\n",
    "!git add .\n",
    "!git commit -m \"Complete homework 2: Movie Poster Generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9c: Push your branch (5 pts)\n",
    "\n",
    "!git push -u origin homework-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9d. Create and Merge the Pull Request\n",
    "\n",
    "Now go to your repository on GitHub (https://github.com/YOUR-USERNAME/ai-engineering-fordham):\n",
    "\n",
    "1. You should see a banner saying **\"homework-2 had recent pushes\"** - click **\"Compare & pull request\"**\n",
    "2. Give your PR a title: `\"Homework 2: Movie Poster Generator\"`\n",
    "3. Click **\"Create pull request\"**\n",
    "4. Review your changes in the PR\n",
    "5. Click **\"Merge pull request\"** then **\"Confirm merge\"**\n",
    "\n",
    "**Your PR should now show as \"Merged\"** - this is what the TA will check!\n",
    "\n",
    "Run the cell below to verify your branch was merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your PR was merged (run after merging on GitHub)\n",
    "!git checkout main\n",
    "!git pull\n",
    "!git log --oneline -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BONUS: Full Pipeline (10 bonus points)\n",
    "\n",
    "Put everything together! Create a complete pipeline that takes a movie description and returns both the structured data AND a generated poster.\n",
    "\n",
    "**Challenge:** Write your own original movie description and generate a poster for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Create a complete pipeline (10 bonus pts)\n",
    "\n",
    "async def movie_to_poster(description: str) -> tuple[Movie, str]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: description -> structured data -> poster\n",
    "    \n",
    "    Args:\n",
    "        description: A text description of a movie\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (Movie object, path to saved poster image)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with YOUR OWN original movie idea!\n",
    "\n",
    "my_movie_description = \"\"\"\n",
    "YOUR ORIGINAL MOVIE IDEA HERE - BE CREATIVE!\n",
    "Describe the plot, characters, setting, visual style, and mood.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to run:\n",
    "# movie, poster_path = await movie_to_poster(my_movie_description)\n",
    "# print(f\"Generated poster for: {movie.title}\")\n",
    "# print(movie.model_dump_json(indent=2))\n",
    "# display(Image(poster_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] Your `Movie` schema includes all required fields with proper validation\n",
    "- [ ] `extract_movie()` returns a valid `Movie` object\n",
    "- [ ] Async processing works and shows timing\n",
    "- [ ] Temperature comparison shows deterministic vs random outputs\n",
    "- [ ] Logprobs visualization works and displays token probabilities\n",
    "- [ ] Reasoning model comparison shows differences between o3-mini and gpt-5-mini\n",
    "- [ ] You generated and displayed at least one movie poster\n",
    "- [ ] Created branch `homework-2` and pushed to GitHub\n",
    "- [ ] Opened a Pull Request from `homework-2` to `main`\n",
    "- [ ] **Merged the PR** (it should show as \"Merged\" on GitHub)\n",
    "- [ ] Submitted notebook on Blackboard\n",
    "\n",
    "**Submission:**\n",
    "1. Complete all tasks in this notebook\n",
    "2. Create a PR and **merge it** on GitHub\n",
    "3. Submit your notebook (`.ipynb` file) on **Blackboard**\n",
    "\n",
    "**The TA will verify your submission by checking the merged PR on your GitHub repo.**\n",
    "\n",
    "---\n",
    "\n",
    "**Great work!** You've built a complete AI-powered application, explored LLM parameters and reasoning, and learned a professional Git workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
