{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Homework 3**: Improving Lexical Search\n",
    "\n",
    "### ðŸ“… **Due Date**: Day of Lecture 4, 11:59 PM\n",
    "\n",
    "\n",
    "**Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "You'll apply what we covered in Lecture 3 (Lexical Search & BM25) to a real e-commerce search problem using the **WANDS dataset** \n",
    "- WANDS stands for Wayfair Annotated Dataset. It's a dataset of furniture products and search queries, along with human relevance judgments.\n",
    "\n",
    "You will:\n",
    "1. **Build a search engine** from scratch using BM25.\n",
    "2. **Learn how to evaluate search results** using NDCG â€” a metric for measuring search quality\n",
    "3. **Attempt to improve your search engine** by adding multiple fields\n",
    "4. **Use LLMs to improve your search engine** by adding simple query understanding\n",
    "\n",
    "Yes, *you* will do all these things. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Environment Setup\n",
    "\n",
    "First, let's set up your environment and verify everything works.\n",
    "\n",
    "### 1a. Install dependencies and verify imports\n",
    "\n",
    "Run `uv add pystemmer` in your terminal to add the Snowball stemmer. Then run the cell below to verify all imports work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1a: Verify imports work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "from pathlib import Path\n",
    "# a stemmer from `pystemmer` for better tokenization\n",
    "import Stemmer \n",
    "# llm packages\n",
    "import litellm\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Pandas display settings\n",
    "# pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Ignore pydantic warnings for litellm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 1b. Verify API keys\n",
    "\n",
    "Test that your API keys work by making a simple call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1b: Verify API keys\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'API working!' and nothing else.\"}],\n",
    "    max_tokens=20\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Load and Explore the WANDS Dataset\n",
    "\n",
    "The **WANDS dataset** (Wayfair Annotated Dataset) contains:\n",
    "- 43K furniture products from Wayfair\n",
    "- 480 real search queries\n",
    "- 233K human relevance judgments (query-product pairs)\n",
    "\n",
    "This is a real-world search benchmark used to evaluate e-commerce search systems!\n",
    "\n",
    "**Data Source**: [WANDS on GitHub](https://github.com/wayfair/WANDS)\n",
    "\n",
    "The data files are pre-downloaded in the `data/` directory:\n",
    "- `wayfair-products.csv` - Product catalog\n",
    "- `wayfair-queries.csv` - Search queries\n",
    "- `wayfair-labels.csv` - Relevance judgments\n",
    "\n",
    "### Data Loading Functions (provided)\n",
    "\n",
    "Run the cell below to define the loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions (provided)\n",
    "# Note: Data from WANDS (Wayfair Annotated Dataset)\n",
    "# Source: https://github.com/wayfair/WANDS\n",
    "\n",
    "def load_wands_products(data_dir: str = \"../data\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load WANDS products from local file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the data directory containing wayfair-products.csv\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with product information including product_id, product_name,\n",
    "        product_class, category_hierarchy, product_description, etc.\n",
    "    \"\"\"\n",
    "    filepath = Path(data_dir) / \"wayfair-products.csv\"\n",
    "    products = pd.read_csv(filepath, sep='\\t')\n",
    "    products = products.rename(columns={'category hierarchy': 'category_hierarchy'})\n",
    "    return products\n",
    "\n",
    "def load_wands_queries(data_dir: str = \"../data\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load WANDS queries from local file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the data directory containing wayfair-queries.csv\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with query_id and query columns\n",
    "    \"\"\"\n",
    "    filepath = Path(data_dir) / \"wayfair-queries.csv\"\n",
    "    queries = pd.read_csv(filepath, sep='\\t')\n",
    "    return queries\n",
    "\n",
    "def load_wands_labels(data_dir: str = \"../data\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load WANDS relevance labels from local file.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to the data directory containing wayfair-labels.csv\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with query_id, product_id, label (Exact/Partial/Irrelevant),\n",
    "        and grade (2/1/0) columns\n",
    "    \"\"\"\n",
    "    filepath = Path(data_dir) / \"wayfair-labels.csv\"\n",
    "    labels = pd.read_csv(filepath, sep='\\t')\n",
    "    grade_map = {'Exact': 2, 'Partial': 1, 'Irrelevant': 0}\n",
    "    labels['grade'] = labels['label'].map(grade_map)\n",
    "    return labels\n",
    "\n",
    "print(\"Loading functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2a. Load the data\n",
    "\n",
    "Use the provided functions to load all three datasets. Print the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2a: Load the data\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2b. Explore products\n",
    "\n",
    "List the available columns, and display a few sample products. \n",
    "\n",
    "Which columns might be useful for search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 2c. Understand relevance judgments\n",
    "\n",
    "The `labels` dataset contains human judgments of relevance. In particular, for each query-product pair, it contains:\n",
    "| Label        | Grade | Description                                 |\n",
    "|--------------|-------|---------------------------------------------|\n",
    "| Exact        |   2   | This product is exactly what the user wants |\n",
    "| Partial      |   1   | This product is somewhat relevant           |\n",
    "| Irrelevant   |   0   | This product doesn't match the query        |\n",
    "\n",
    "First, let's look at the distribution of grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2c: Understand judgments\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Build and Run BM25 Search\n",
    "\n",
    "Now let's build a BM25 search engine! We'll use the same concepts from Lecture 3.\n",
    "\n",
    "### Provided Functions\n",
    "\n",
    "We're giving you these functions to work with. Run the next cell to define them, then look at the examples.\n",
    "\n",
    "| Function | What it does |\n",
    "|----------|--------------|\n",
    "| `snowball_tokenize(text)` | Tokenizes text, removes punctuation, stems words |\n",
    "| `build_index(docs, tokenizer)` | Builds an inverted index from a list of documents |\n",
    "| `get_tf(term, doc_id, index)` | Gets term frequency for a term in a document |\n",
    "| `get_df(term, index)` | Gets document frequency for a term (how many docs contain the term) |\n",
    "| `bm25_idf(df, num_docs)` | Calculates the IDF component of BM25 |\n",
    "| `bm25_tf(tf, doc_len, avg_doc_len)` | Calculates the TF normalization for BM25 |\n",
    "| `score_bm25(query, index, ...)` | Scores all documents for a query using BM25 |\n",
    "| `search_products(query, ...)` | Searches and returns top-k results |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided functions - run this cell to define them\n",
    "\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "punct_trans = str.maketrans({key: ' ' for key in string.punctuation})\n",
    "\n",
    "def snowball_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text with Snowball stemming.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to tokenize\n",
    "        \n",
    "    Returns:\n",
    "        List of stemmed tokens\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return []\n",
    "    text = str(text).translate(punct_trans)\n",
    "    tokens = text.lower().split()\n",
    "    return [stemmer.stemWord(token) for token in tokens]\n",
    "\n",
    "def build_index(docs: list[str], tokenizer) -> tuple[dict, list[int]]:\n",
    "    \"\"\"\n",
    "    Build an inverted index from a list of documents.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of document strings to index\n",
    "        tokenizer: Function that takes text and returns list of tokens\n",
    "        \n",
    "    Returns:\n",
    "        index: dict mapping term -> {doc_id: term_count}\n",
    "        doc_lengths: list of document lengths (in tokens)\n",
    "    \"\"\"\n",
    "    index = {}\n",
    "    doc_lengths = []\n",
    "    \n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        tokens = tokenizer(doc)\n",
    "        doc_lengths.append(len(tokens))\n",
    "        term_counts = Counter(tokens)\n",
    "        \n",
    "        for term, count in term_counts.items():\n",
    "            if term not in index:\n",
    "                index[term] = {}\n",
    "            index[term][doc_id] = count\n",
    "    \n",
    "    return index, doc_lengths\n",
    "\n",
    "def get_tf(term: str, doc_id: int, index: dict) -> int:\n",
    "    \"\"\"\n",
    "    Get term frequency for a term in a document.\n",
    "    \n",
    "    Args:\n",
    "        term: The term to look up\n",
    "        doc_id: The document ID\n",
    "        index: The inverted index\n",
    "        \n",
    "    Returns:\n",
    "        Term frequency (count), or 0 if not found\n",
    "    \"\"\"\n",
    "    if term in index and doc_id in index[term]:\n",
    "        return index[term][doc_id]\n",
    "    return 0\n",
    "\n",
    "def get_df(term: str, index: dict) -> int:\n",
    "    \"\"\"\n",
    "    Get document frequency for a term.\n",
    "    \n",
    "    Args:\n",
    "        term: The term to look up\n",
    "        index: The inverted index\n",
    "        \n",
    "    Returns:\n",
    "        Number of documents containing the term\n",
    "    \"\"\"\n",
    "    if term in index:\n",
    "        return len(index[term])\n",
    "    return 0\n",
    "\n",
    "def bm25_idf(df: int, num_docs: int) -> float:\n",
    "    \"\"\"\n",
    "    BM25 IDF formula.\n",
    "    \n",
    "    Args:\n",
    "        df: Document frequency\n",
    "        num_docs: Total number of documents\n",
    "        \n",
    "    Returns:\n",
    "        IDF score\n",
    "    \"\"\"\n",
    "    return np.log((num_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "\n",
    "def bm25_tf(tf: int, doc_len: int, avg_doc_len: float, k1: float = 1.2, b: float = 0.75) -> float:\n",
    "    \"\"\"\n",
    "    BM25 TF normalization.\n",
    "    \n",
    "    Args:\n",
    "        tf: Term frequency\n",
    "        doc_len: Document length in tokens\n",
    "        avg_doc_len: Average document length\n",
    "        k1: Saturation parameter (default 1.2)\n",
    "        b: Length normalization (default 0.75)\n",
    "        \n",
    "    Returns:\n",
    "        Normalized TF score\n",
    "    \"\"\"\n",
    "    return (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))\n",
    "\n",
    "def score_bm25(query: str, index: dict, num_docs: int, doc_lengths: list[int], \n",
    "               tokenizer, k1: float = 1.2, b: float = 0.75) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Score all documents using BM25.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        index: Inverted index\n",
    "        num_docs: Total number of documents\n",
    "        doc_lengths: List of document lengths\n",
    "        tokenizer: Tokenization function\n",
    "        \n",
    "    Returns:\n",
    "        Array of scores for each document\n",
    "    \"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    scores = np.zeros(num_docs)\n",
    "    avg_doc_len = np.mean(doc_lengths) if doc_lengths else 1.0\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        df = get_df(token, index)\n",
    "        if df == 0:\n",
    "            continue\n",
    "        \n",
    "        idf = bm25_idf(df, num_docs)\n",
    "        \n",
    "        if token in index:\n",
    "            for doc_id, tf in index[token].items():\n",
    "                tf_norm = bm25_tf(tf, doc_lengths[doc_id], avg_doc_len, k1, b)\n",
    "                scores[doc_id] += idf * tf_norm\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def search_products(query: str, products_df: pd.DataFrame, index: dict, \n",
    "                    doc_lengths: list[int], tokenizer, k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Search products and return top-k results.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        products_df: DataFrame of products\n",
    "        index: Inverted index\n",
    "        doc_lengths: Document lengths\n",
    "        tokenizer: Tokenization function\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with top-k products and scores\n",
    "    \"\"\"\n",
    "    scores = score_bm25(query, index, len(products_df), doc_lengths, tokenizer)\n",
    "    top_k_idx = np.argsort(-scores)[:k]\n",
    "    \n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results['score'] = scores[top_k_idx]\n",
    "    results['rank'] = range(1, k + 1)\n",
    "    return results\n",
    "\n",
    "print(\"All functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of each function\n",
    "\n",
    "# 1. snowball_tokenize - tokenizes and stems text\n",
    "print(\"1. snowball_tokenize('Running shoes are amazing!')\")\n",
    "print(f\"   -> {snowball_tokenize('Running shoes are amazing!')}\")\n",
    "print(\"   Notice: 'Running' -> 'run', 'shoes' -> 'shoe', 'amazing' -> 'amaz'\")\n",
    "\n",
    "# 2. build_index - builds inverted index (we'll use a tiny example)\n",
    "tiny_docs = [\"red shoe\", \"blue shoe\", \"red hat\"]\n",
    "tiny_index, tiny_lengths = build_index(tiny_docs, snowball_tokenize)\n",
    "print(\"\\n2. build_index(['red shoe', 'blue shoe', 'red hat'], tokenizer)\")\n",
    "print(f\"   Index: {tiny_index}\")\n",
    "print(f\"   Lengths: {tiny_lengths}\")\n",
    "\n",
    "# 3. get_tf - get term frequency\n",
    "print(\"\\n3. get_tf('red', doc_id=0, tiny_index)\")\n",
    "print(f\"   -> {get_tf('red', 0, tiny_index)}  (doc 0 = 'red shoe' has 1 'red')\")\n",
    "\n",
    "# 4. get_df - get document frequency  \n",
    "print(\"\\n4. get_df('red', tiny_index)\")\n",
    "print(f\"   -> {get_df('red', tiny_index)}  ('red' appears in 2 documents)\")\n",
    "\n",
    "# 5. bm25_idf - calculate IDF (rare terms get higher scores)\n",
    "print(\"\\n5. bm25_idf(df=100, num_docs=10000)\")\n",
    "print(f\"   -> {bm25_idf(100, 10000):.4f}  (term in 100 of 10000 docs)\")\n",
    "\n",
    "# 6. bm25_tf - normalize term frequency by document length\n",
    "print(\"\\n6. bm25_tf(tf=3, doc_len=50, avg_doc_len=100)\")\n",
    "print(f\"   -> {bm25_tf(3, 50, 100):.4f}  (short doc gets boosted)\")\n",
    "\n",
    "# 7-8. score_bm25 and search_products - we'll use these next!\n",
    "print(\"\\nWe'll use score_bm25() and search_products() in Task 3a!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 3a. Create BM25 index for product_name\n",
    "\n",
    "Build an inverted index for the `product_name` field and run a sample search for a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3a: Create BM25 index for product_name\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 3b. Add product_description to search\n",
    "\n",
    "Create a second index for `product_description` and combine scores from both fields.\n",
    "\n",
    "**Hint**: You can combine the two scores by adding them together. This is like multi-field search from Lecture 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3b: Add product_description to search\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Measuring Search Quality\n",
    "\n",
    "We built a little search engine. How do we know if it's any good?\n",
    "\n",
    "Consider two search results for \"coffee table\":\n",
    "\n",
    "| Ranking A | Ranking B |\n",
    "|-----------|-----------|\n",
    "| 1. Wooden Coffee Table (Exact) | 1. Metal Lamp (Irrelevant) |\n",
    "| 2. Glass Coffee Table (Exact) | 2. Wooden Coffee Table (Exact) |\n",
    "| 3. Metal Lamp (Irrelevant) | 3. Glass Coffee Table (Exact) |\n",
    "\n",
    "### A. Precision\n",
    "\n",
    "One way to measure the quality of a ranking is to look at the precision within these first 3 results. \n",
    "- Precision is the ratio of relevant results to total results at position k.\n",
    "- We call this precision@3, and more generally precision@k is the ratio of relevant results to total results at position k.\n",
    "  \n",
    "In this scenario, if we consider \"exact\" results as relevant, then both rankings have precision@3 = 2/3.\n",
    "\n",
    "### B. DCG\n",
    "\n",
    "Both rankings have the same precision, but Ranking A is clearly better \n",
    "- users look at results from the top down, and most people never scroll past the first few results\n",
    "- as such, rankings that return relevant results earlier are better\n",
    "\n",
    "So we need a metric that rewards **relevant** results, and rewards them **more** when they appear at the **top**\n",
    "\n",
    "NDCG (Normalized Discounted Cumulative Gain) does this by giving each result a \"gain\" based on its relevance, then **discounting** that gain based on position.\n",
    "\n",
    "**The formula** for each result at position $i$:\n",
    "\n",
    "$$\\text{gain}_i = \\frac{2^{\\text{relevance}} - 1}{\\log_2(i + 1)}$$\n",
    "\n",
    "- **Numerator** $(2^{\\text{relevance}} - 1)$: How relevant is this result?\n",
    "  - Irrelevant (0): $2^0 - 1 = 0$ (no gain)\n",
    "  - Partial (1): $2^1 - 1 = 1$ (some gain)\n",
    "  - Exact (2): $2^2 - 1 = 3$ (lots of gain!)\n",
    "  \n",
    "- **Denominator** $\\log_2(i + 1)$: The \"discount\" based on position\n",
    "  - Position 1: $\\log_2(2) = 1$ (no discount)\n",
    "  - Position 2: $\\log_2(3) = 1.58$ (small discount)\n",
    "  - Position 10: $\\log_2(11) = 3.46$ (bigger discount)\n",
    "\n",
    "**DCG** sums the discounted score for each result\n",
    "\n",
    "$$\\text{DCG} = \\sum_{i=1}^{k} \\frac{2^{\\text{relevance}_i} - 1}{\\log_2(i + 1)}$$\n",
    "\n",
    "### 3. NDCG: Normalized DCG\n",
    "\n",
    "One problem with DCG is that the score depends on how many relevant products exist. \n",
    "- A query with 10 exact matches will have a higher DCG than one with only 2, even if both rankings are \"perfect.\"\n",
    "\n",
    "One solution is to normalize by the *ideal* DCG â€” what the score would be if we ranked everything perfectly (all relevant results at the top).\n",
    "\n",
    "$$\\text{NDCG} = \\frac{\\text{DCG}}{\\text{Ideal DCG}}$$\n",
    "\n",
    "- **NDCG = 1.0**: Perfect -- best possible order\n",
    "- **NDCG = 0.5**: OK -- some good some bad\n",
    "- **NDCG = 0.0**: Worst -- results are irrelevant\n",
    "\n",
    "**Read the above carefully.** In the next cell, explain in your own words: why does the discount formula use $\\log_2$? What happens to results at position 1 vs position 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4a: Answer in a comment\n",
    "# Why does DCG use log2 for the discount? What's the effect on position 1 vs position 10?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 4b. Calculate NDCG by hand\n",
    "\n",
    "Let's work through an example step by step.\n",
    "\n",
    "**Scenario**: You search for \"wooden coffee table\" and get these results:\n",
    "\n",
    "| Position | Product | Relevance |\n",
    "|----------|---------|----------|\n",
    "| 1 | Glass Coffee Table | Partial (1) |\n",
    "| 2 | Wooden Coffee Table | Exact (2) |\n",
    "| 3 | Wooden Side Table | Partial (1) |\n",
    "| 4 | Metal Coffee Table | Irrelevant (0) |\n",
    "| 5 | Wooden Coffee Table (different) | Exact (2) |\n",
    "\n",
    "**Your task**: Calculate DCG and NDCG@5 by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4b: Calculate NDCG by hand\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 4c. Implement NDCG function\n",
    "\n",
    "Now implement the NDCG calculation in code. Verify your implementation matches your hand calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4c: Implement NDCG function\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your implementation matches your hand calculation\n",
    "test_relevances = [1, 2, 1, 0, 2]\n",
    "\n",
    "dcg = calculate_dcg(test_relevances, k=5)\n",
    "ndcg = calculate_ndcg(test_relevances, k=5)\n",
    "\n",
    "print(f\"DCG@5 = {dcg:.4f}\")\n",
    "print(f\"NDCG@5 = {ndcg:.4f}\")\n",
    "\n",
    "# These should match your hand calculations from Task 4b!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Evaluate Your Search Strategy\n",
    "\n",
    "Now let's evaluate our BM25 search across all queries in the WANDS dataset.\n",
    "\n",
    "### Evaluation Helper Functions (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation helper functions (provided)\n",
    "\n",
    "def get_relevance_grades(product_ids: list[int], query_id: int, labels_df: pd.DataFrame) -> list[int]:\n",
    "    \"\"\"\n",
    "    Get relevance grades for a list of product IDs given a query.\n",
    "    \n",
    "    Args:\n",
    "        product_ids: List of product IDs in rank order\n",
    "        query_id: The query ID\n",
    "        labels_df: DataFrame with relevance labels\n",
    "        \n",
    "    Returns:\n",
    "        List of relevance grades (0, 1, or 2) for each product\n",
    "    \"\"\"\n",
    "    # Get labels for this query\n",
    "    query_labels = labels_df[labels_df['query_id'] == query_id]\n",
    "    label_dict = dict(zip(query_labels['product_id'], query_labels['grade']))\n",
    "    \n",
    "    # Look up grades for each product (default to 0 if not labeled)\n",
    "    return [label_dict.get(pid, 0) for pid in product_ids]\n",
    "\n",
    "def evaluate_single_query(query_text: str, query_id: int, products_df: pd.DataFrame,\n",
    "                          labels_df: pd.DataFrame, search_func, k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate search for a single query.\n",
    "    \n",
    "    Args:\n",
    "        query_text: The search query text\n",
    "        query_id: The query ID for looking up labels\n",
    "        products_df: DataFrame of products\n",
    "        labels_df: DataFrame with relevance labels\n",
    "        search_func: Function that takes query and returns DataFrame with product_id column\n",
    "        k: Number of results to consider\n",
    "        \n",
    "    Returns:\n",
    "        NDCG@k score for this query\n",
    "    \"\"\"\n",
    "    results = search_func(query_text)\n",
    "    product_ids = results['product_id'].tolist()[:k]\n",
    "    relevances = get_relevance_grades(product_ids, query_id, labels_df)\n",
    "    return calculate_ndcg(relevances, k)\n",
    "\n",
    "def evaluate_search(search_func, products_df: pd.DataFrame, queries_df: pd.DataFrame,\n",
    "                    labels_df: pd.DataFrame, k: int = 10, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate search across all queries.\n",
    "    \n",
    "    Args:\n",
    "        search_func: Function that takes query string and returns DataFrame with product_id\n",
    "        products_df: DataFrame of products\n",
    "        queries_df: DataFrame of queries\n",
    "        labels_df: DataFrame with relevance labels\n",
    "        k: Number of results to consider\n",
    "        verbose: Whether to print progress\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with query_id, query, and ndcg columns\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for _, row in queries_df.iterrows():\n",
    "        query_id = row['query_id']\n",
    "        query_text = row['query']\n",
    "        \n",
    "        ndcg = evaluate_single_query(query_text, query_id, products_df, \n",
    "                                     labels_df, search_func, k)\n",
    "        results.append({\n",
    "            'query_id': query_id,\n",
    "            'query': query_text,\n",
    "            'ndcg': ndcg\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Evaluated {len(results_df)} queries\")\n",
    "        print(f\"Mean NDCG@{k}: {results_df['ndcg'].mean():.4f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 5a. Run evaluation on all queries\n",
    "\n",
    "Create a search function and evaluate it on all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5a: Run evaluation on all queries\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 5b. Identify failing queries\n",
    "\n",
    "Find queries where our search performed poorly (NDCG = 0 or very low). Analyze one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5b: Identify failing queries\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### 5c. Analyze the distribution\n",
    "\n",
    "Visualize the distribution of NDCG scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5c: Analyze the distribution\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Improve Search with Additional Fields\n",
    "\n",
    "Our baseline only searches the `product_name` field. Let's improve by adding more fields!\n",
    "\n",
    "### 6a. Index product_class field\n",
    "\n",
    "The `product_class` field contains the category of the product (e.g., \"Rugs\", \"Coffee Tables\"). This is a powerful signal!\n",
    "\n",
    "Create a search function that combines all three fields (name, description, class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6a: Index product_class field\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 6b. Evaluate three-field search\n",
    "\n",
    "Now evaluate your three-field search on all queries to see how it compares to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6b: Evaluate three-field search\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 6c. Compare to baseline\n",
    "\n",
    "Analyze which queries improved and which degraded when using three-field search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6c: Compare to baseline\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 7: Query Understanding with LLM\n",
    "\n",
    "Sometimes users search for \"star wars rug\" when they really want a \"rug with Star Wars theme\". An LLM can help us understand what the user is actually looking for!\n",
    "\n",
    "### 7a. Extract product type from query\n",
    "\n",
    "Write a function using LiteLLM with structured outputs (Pydantic) to extract key information from a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7a: Extract product type, theme, material, color, and any other information you deem relevcant from the query\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your query understanding function by running it against these test queries\n",
    "test_queries = [\n",
    "    \"star wars rug\",\n",
    "    \"wooden coffee table\",\n",
    "    \"blue leather sofa\",\n",
    "    \"modern metal bookshelf\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### 7b. Create an LLM-enhanced search\n",
    "\n",
    "Use the extracted product type to boost matching results. If the LLM identifies \"rug\" as the product type, boost products where `product_class` contains \"rug\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7b: Create an LLM-enhanced search\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 8: Submit via Pull Request\n",
    "\n",
    "Now let's submit your work using the Git workflow from previous homeworks.\n",
    "- [ ] Create a new branch called `homework-3`\n",
    "- [ ] Commit you work and push it to the branch\n",
    "- [ ] Create a PR with a nice description of your changes\n",
    "- [ ] Merge the PR to your main branch\n",
    "  \n",
    "**The TA will verify your submission by checking the merged PR on your GitHub repo.**\n",
    "\n",
    "**Also remember to submit your homework on Blackboard!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ce)",
   "language": "python",
   "name": "ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
