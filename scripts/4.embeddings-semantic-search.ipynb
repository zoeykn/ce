{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Embeddings & Semantic Search\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In this lecture, we'll explore **embeddings and semantic search**. These tools complement the lexical search tools that we covered last week.\n",
    "\n",
    "By the end of this session, you'll be able to:\n",
    "- Understand what embeddings are and how they encode meaning\n",
    "- Use both local (Hugging Face) and API-based (OpenAI) embedding models\n",
    "- Implement semantic search from scratch using cosine similarity\n",
    "- Discover why similarity does not equal relevance\n",
    "- Build hybrid search combining BM25 + embeddings\n",
    "- Compare search approaches using NDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using modules in your code\n",
    "\n",
    "Starting today, we'll use a **helpers module** to organize reusable code.\n",
    "\n",
    "Instead of copying functions between notebooks, we **import** them:\n",
    "\n",
    "```python\n",
    "from helpers import load_wands_products, snowball_tokenize, score_bm25\n",
    "```\n",
    "\n",
    "This is how professional codebases work:\n",
    "- **Single source of truth** - fix a bug once, fixed everywhere\n",
    "- **Cleaner notebooks** - focus on the lesson, not boilerplate\n",
    "- **Reusability** - same functions work across lectures and homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import from our helpers module!\n",
    "from helpers import (\n",
    "    # Data loading\n",
    "    load_wands_products, load_wands_queries, load_wands_labels,\n",
    "    # BM25 (from Lecture 3)\n",
    "    build_index, score_bm25, search_bm25,\n",
    "    # Evaluation\n",
    "    evaluate_search,\n",
    "    # Embeddings\n",
    "    get_embedding_openai, get_embedding_local, batch_embed_local,\n",
    "    # Similarity\n",
    "    cosine_similarity, batch_cosine_similarity,\n",
    "    # Utility\n",
    "    get_product_sample, normalize_scores\n",
    ")\n",
    "\n",
    "# Load environment variables for API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. From Keywords to Meaning\n",
    "\n",
    "## 1.1 Recap: BM25 and Lexical Search\n",
    "\n",
    "Last week, we built a search engine using **BM25** - a lexical search algorithm that:\n",
    "- Matches documents based on **exact token matches**\n",
    "- Uses **TF-IDF** scoring with saturation and length normalization\n",
    "- Gives you **precise control** over what matches\n",
    "\n",
    "Let's reload our WANDS data and BM25 index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WANDS dataset (same as Lecture 3 and Homework 3)\n",
    "products = load_wands_products()\n",
    "queries = load_wands_queries()\n",
    "labels = load_wands_labels()\n",
    "\n",
    "print(f\"Products: {len(products):,}\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(f\"Labels: {len(labels):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index on product names\n",
    "name_index, name_lengths = build_index(products['product_name'].tolist())\n",
    "print(f\"Index contains {len(name_index):,} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The Limitation: Exact Token Matching\n",
    "\n",
    "BM25 is powerful, but it has a fundamental limitation: it only matches **exact tokens**.\n",
    "\n",
    "What happens when we search for synonyms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"couch\"\n",
    "couch_results = search_bm25(\"couch\", name_index, products, name_lengths, k=5)\n",
    "print(\"BM25 results for 'couch':\")\n",
    "couch_results[['product_name', 'bm25_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"sofa\" - a synonym!\n",
    "sofa_results = search_bm25(\"sofa\", name_index, products, name_lengths, k=5)\n",
    "print(\"BM25 results for 'sofa':\")\n",
    "sofa_results[['product_name', 'bm25_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the problem:**\n",
    "- \"couch\" results contain products with \"couch\" in the name\n",
    "- \"sofa\" results contain products with \"sofa\" in the name\n",
    "- But they're **synonyms** - a user searching for \"couch\" would probably want sofas too!\n",
    "\n",
    "BM25 treats them as completely different words because it only matches exact tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Lexical vs Semantic Search\n",
    "\n",
    "| Aspect | Lexical Search (BM25) | Semantic Search (Embeddings) |\n",
    "|--------|----------------------|-----------------------------|\n",
    "| **How it works** | Exact token matching | Meaning-based similarity |\n",
    "| **Synonyms** | Misses them | Finds them! |\n",
    "| **Control** | High - you decide what matches | Lower - model decides |\n",
    "| **Speed** | Very fast (inverted index) | Slower (vector comparisons) |\n",
    "| **Best for** | Precise queries, keywords, IDs | Fuzzy queries, concepts |\n",
    "\n",
    "**Key insight**: Lexical search is a **scalpel** - precise but limited. Semantic search is a **net** - catches more but less control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. What are embeddings?\n",
    "\n",
    "> **TERM: Embedding**  \n",
    "> A **dense vector representation** that maps text (or other data) to a point in high-dimensional space where **semantically similar items are close together**.\n",
    "\n",
    "Think of it as assigning \"coordinates\" to the **meaning** of text:\n",
    "- \"couch\" and \"sofa\" would have similar coordinates (close together)\n",
    "- \"couch\" and \"refrigerator\" would have different coordinates (far apart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Getting Your First Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an embedding using OpenAI's API\n",
    "couch_emb = get_embedding_openai(text=\"couch\", model=\"text-embedding-3-small\")\n",
    "\n",
    "print(f\"Type: {type(couch_emb)}\")\n",
    "print(f\"Dimension: {len(couch_emb)}\")\n",
    "print(f\"First 10 values: {couch_emb[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is a **1536-dimensional vector** of floating-point numbers.\n",
    "\n",
    "Each dimension captures some aspect of the word's meaning - but unlike features we design ourselves, these are **latent features** learned by the model.\n",
    "\n",
    "> **TERM: Latent Features**  \n",
    "> Hidden dimensions in the embedding that capture abstract concepts. They're not directly interpretable like \"color=red\" or \"size=large\" - they're patterns the model discovered during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Embeddings Capture Meaning\n",
    "\n",
    "Let's see how embeddings capture the relationship between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for related words\n",
    "words = [\"couch\", \"sofa\", \"chair\", \"table\", \"refrigerator\"]\n",
    "embeddings = {word: get_embedding_openai(text=word, model=\"text-embedding-3-large\") for word in words}\n",
    "\n",
    "# Calculate similarity between all pairs\n",
    "print(\"Similarity matrix:\")\n",
    "\n",
    "\n",
    "word_width = max(len(w) for w in words) + 2\n",
    "# Header row: empty cell + each word as column header (each in word_width)\n",
    "header = \"\".join([f\"{'':<{word_width}s}\"] + [f\"{w:>{word_width}s}\" for w in words])\n",
    "print(\"Similarity matrix:\")\n",
    "print(header)\n",
    "print(\"-\" * (word_width + len(words) * word_width))\n",
    "# Data rows: row label + scores, each column word_width wide\n",
    "for w1 in words:\n",
    "    row = [f\"{w1:<{word_width}s}\"]\n",
    "    for w2 in words:\n",
    "        sim = cosine_similarity(embeddings[w1], embeddings[w2])\n",
    "        row.append(f\"{sim:>{word_width}.2f}\")\n",
    "    print(\"\".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice?**\n",
    "- \"couch\" and \"sofa\" have **very high similarity** (~0.75) - the model learned they're synonyms!\n",
    "- Furniture items (couch, sofa, chair, table) are more similar to each other\n",
    "- \"refrigerator\" is less similar to the furniture items\n",
    "\n",
    "The embedding model learned these relationships from training on massive amounts of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for related words\n",
    "words_2 = [\"Apostolos Filippas\", \"Tilda Swinton\", \"Technology\", \"Movies\", \"Suspiria\", \"Greek\", \"British\"]\n",
    "embeddings_2 = {word: get_embedding_openai(text=word, model=\"text-embedding-3-large\") for word in words_2}\n",
    "\n",
    "# Calculate similarity between all pairs\n",
    "print(\"Similarity matrix:\")\n",
    "\n",
    "\n",
    "word_width = max(len(w) for w in words_2) + 2\n",
    "# Header row: empty cell + each word as column header (each in word_width)\n",
    "header = \"\".join([f\"{'':<{word_width}s}\"] + [f\"{w:>{word_width}s}\" for w in words_2])\n",
    "print(\"Similarity matrix:\")\n",
    "print(header)\n",
    "print(\"-\" * (word_width + len(words_2) * word_width))\n",
    "# Data rows: row label + scores, each column word_width wide\n",
    "for w1 in words_2:\n",
    "    row = [f\"{w1:<{word_width}s}\"]\n",
    "    for w2 in words_2:\n",
    "        sim = cosine_similarity(embeddings_2[w1], embeddings_2[w2])\n",
    "        row.append(f\"{sim:>{word_width}.2f}\")\n",
    "    print(\"\".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2.5 Local vs API Embeddings\n",
    "\n",
    "> **TERM: Hugging Face**  \n",
    "> An open-source platform hosting thousands of pre-trained AI models. Think of it as \"GitHub for AI models\" - you can download and run models locally without API calls or costs.\n",
    "\n",
    "So far we've used OpenAI's embedding API. But there's another option: **run models locally**!\n",
    "\n",
    "## 2.5.1 Loading a Local Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a local embedding - first call downloads the model (~80MB)\n",
    "local_emb = get_embedding_local(\"wooden coffee table\")\n",
    "\n",
    "print(f\"Local embedding dimension: {len(local_emb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dimensions\n",
    "api_emb = get_embedding_openai(\"wooden coffee table\")\n",
    "\n",
    "print(f\"OpenAI (API): {len(api_emb)} dimensions\")\n",
    "print(f\"MiniLM (Local): {len(local_emb)} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.3 Trade-offs: API vs Local\n",
    "\n",
    "| Aspect | API (OpenAI) | Local (Hugging Face) |\n",
    "|--------|-------------|---------------------|\n",
    "| **Cost** | ~$0.02 per 1M tokens | FREE |\n",
    "| **Dimensions** | 1536 (more expressive) | 384 (more compact) |\n",
    "| **Quality** | Generally higher | Good for most tasks |\n",
    "| **Speed** | Network latency | Faster for batches |\n",
    "| **Privacy** | Data sent to API | Data stays local |\n",
    "| **Setup** | Just API key | Downloads model (~80MB) |\n",
    "\n",
    "**When to use which?**\n",
    "- **Prototyping/Learning**: Local - free experimentation!\n",
    "- **Production with privacy needs**: Local\n",
    "- **Production needing best quality**: API\n",
    "- **High volume, cost-sensitive**: Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Measuring Similarity with Cosine\n",
    "\n",
    "## 3.1 Why Cosine Similarity?\n",
    "\n",
    "To find similar items, we need to measure the \"distance\" between embeddings. **Cosine similarity** measures the angle between two vectors:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\times \\|b\\|}$$\n",
    "\n",
    "- **1.0** = identical direction (most similar)\n",
    "- **0.0** = perpendicular (unrelated)\n",
    "- **-1.0** = opposite direction (most dissimilar)\n",
    "\n",
    "Why cosine instead of Euclidean distance? Cosine focuses on **direction** (meaning) not **magnitude** (length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cosine_similarity function we imported:\n",
    "def cosine_similarity_manual(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Verify it matches\n",
    "sim1 = cosine_similarity(embeddings[\"couch\"], embeddings[\"sofa\"])\n",
    "sim2 = cosine_similarity_manual(embeddings[\"couch\"], embeddings[\"sofa\"])\n",
    "print(f\"From helpers: {sim1:.6f}\")\n",
    "print(f\"Manual: {sim2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Batch Similarity for Efficiency\n",
    "\n",
    "When searching, we need to compare one query against **thousands of products**. Doing this one-by-one is slow. Instead, we use **matrix operations**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all word embeddings into a matrix\n",
    "word_matrix = np.array([embeddings[w] for w in words])\n",
    "print(f\"Matrix shape: {word_matrix.shape}\")\n",
    "\n",
    "# Query embedding\n",
    "query_emb = embeddings[\"couch\"]\n",
    "\n",
    "# Calculate similarity to all words at once\n",
    "similarities = batch_cosine_similarity(query_emb, word_matrix)\n",
    "\n",
    "for word, sim in zip(words, similarities):\n",
    "    print(f\"{word:15s}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Building Semantic Search from Scratch\n",
    "\n",
    "Now let's build a working semantic search engine!\n",
    "\n",
    "## 4.1 The Semantic Search Pipeline\n",
    "\n",
    "1. **Embed all products** (offline, once)\n",
    "2. **Embed the query** (at search time)\n",
    "3. **Calculate similarity** between query and all products\n",
    "4. **Return top-k** most similar products\n",
    "\n",
    "## 4.2 Embedding Products\n",
    "\n",
    "For speed in class, we'll work with a **sample of 5,000 products**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get consistent sample (same for everyone)\n",
    "products_sample = get_product_sample(products, n=5000)\n",
    "print(f\"Working with {len(products_sample):,} products\")\n",
    "products_sample[['product_id', 'product_name', 'product_class']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text for embedding: combine name and class\n",
    "products_sample['embed_text'] = (\n",
    "    products_sample['product_name'].fillna('') + ' ' +\n",
    "    products_sample['product_class'].fillna('')\n",
    ")\n",
    "\n",
    "products_sample['embed_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all products using local model\n",
    "# This took me 3.5 seconds\n",
    "print(\"Embedding products...\")\n",
    "start = time.time()\n",
    "product_embeddings = batch_embed_local(\n",
    "    products_sample['embed_text'].tolist(),\n",
    "    show_progress=True\n",
    ")\n",
    "print(f\"Done in {time.time() - start:.1f}s\")\n",
    "print(f\"Embeddings shape: {product_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings so we don't have to recompute\n",
    "np.save('temp/product_embeddings_sample.npy', product_embeddings)\n",
    "products_sample.to_csv('temp/products_sample.csv', index=False)\n",
    "print(\"Saved embeddings and sample to 'scripts/temp/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Implementing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_local(query, product_embeddings, products_df, k=10):\n",
    "    \"\"\"Search products using local embedding similarity.\"\"\"\n",
    "    # 1. Embed the query\n",
    "    query_emb = get_embedding_local(query)\n",
    "    \n",
    "    # 2. Calculate similarity to all products\n",
    "    similarities = batch_cosine_similarity(query_emb, product_embeddings)\n",
    "    \n",
    "    # 3. Get top-k indices\n",
    "    top_k_idx = np.argsort(-similarities)[:k]\n",
    "    \n",
    "    # 4. Build results DataFrame\n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results['similarity'] = similarities[top_k_idx]\n",
    "    results['rank'] = range(1, k + 1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search!\n",
    "results = semantic_search_local(\"couch\", product_embeddings, products_sample)\n",
    "results[['rank', 'product_name', 'product_class', 'similarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the synonym problem that BM25 couldn't solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index for the sample\n",
    "sample_index, sample_lengths = build_index(products_sample['product_name'].tolist())\n",
    "\n",
    "# Search for \"sofa\" with BM25\n",
    "bm25_results = search_bm25(\"sofa\", sample_index, products_sample, sample_lengths, k=10)\n",
    "print(\"BM25 for 'sofa':\")\n",
    "print(bm25_results[['product_name', 'bm25_score']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Search for \"sofa\" with semantic search\n",
    "sem_results = semantic_search_local(\"sofa\", product_embeddings, products_sample, k=10)\n",
    "print(\"Semantic for 'sofa':\")\n",
    "print(sem_results[['product_name', 'similarity']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic search finds both \"sofa\" AND \"couch\" products!** It understands they're related concepts.\n",
    "\n",
    "Let's try another query that BM25 struggles with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A conceptual query - no exact keyword match\n",
    "query = \"place to sit and relax\"\n",
    "\n",
    "bm25_results = search_bm25(query, sample_index, products_sample, sample_lengths, k=5)\n",
    "print(f\"BM25 for '{query}':\")\n",
    "print(bm25_results[['product_name', 'bm25_score']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "sem_results = semantic_search_local(query, product_embeddings, products_sample, k=5)\n",
    "print(f\"Semantic for '{query}':\")\n",
    "print(sem_results[['product_name', 'similarity']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. The Critical Lesson: Similarity is NOT Relevance\n",
    "\n",
    "Semantic search seems magical - it finds synonyms and understands concepts! But there's a **critical problem** you must understand.\n",
    "\n",
    "## 5.1 The Domain Mismatch Problem\n",
    "\n",
    "The embedding model was trained on **general web text** (Wikipedia, books, etc.). It learned what words mean in general.\n",
    "\n",
    "But **relevance** in e-commerce search is domain-specific:\n",
    "- A user searching for \"star wars rug\" wants a **rug** with a Star Wars theme\n",
    "- They don't want a Star Wars **poster** or **blanket** - even though those are semantically similar!\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"star wars rug\"\n",
    "query = \"star wars rug\"\n",
    "\n",
    "sem_results = semantic_search_local(query, product_embeddings, products_sample, k=10)\n",
    "print(f\"Semantic search for '{query}':\")\n",
    "sem_results[['rank', 'product_name', 'product_class', 'similarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened?**\n",
    "\n",
    "The semantic search found items that are **similar to \"star wars rug\"** - but many of them might not be rugs at all!\n",
    "\n",
    "The embedding model doesn't understand that:\n",
    "- **\"rug\"** is the **product type** (must match)\n",
    "- **\"star wars\"** is the **theme** (nice to have)\n",
    "\n",
    "It treats all words equally in terms of meaning similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Measuring the Problem with NDCG\n",
    "\n",
    "Let's quantify how well each search method performs using NDCG (from Homework 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter queries to those with products in our sample\n",
    "sample_product_ids = set(products_sample['product_id'])\n",
    "sample_labels = labels[labels['product_id'].isin(sample_product_ids)]\n",
    "sample_query_ids = set(sample_labels['query_id'])\n",
    "sample_queries = queries[queries['query_id'].isin(sample_query_ids)]\n",
    "\n",
    "print(f\"Queries with products in sample: {len(sample_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BM25 on sample\n",
    "print(\"Evaluating BM25...\")\n",
    "bm25_eval = evaluate_search(\n",
    "    lambda q: search_bm25(q, sample_index, products_sample, sample_lengths, k=10),\n",
    "    products_sample, sample_queries, sample_labels, k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Semantic Search on sample\n",
    "print(\"Evaluating Semantic Search...\")\n",
    "semantic_eval = evaluate_search(\n",
    "    lambda q: semantic_search_local(q, product_embeddings, products_sample, k=10),\n",
    "    products_sample, sample_queries, sample_labels, k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare!\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "print(f\"BM25 Mean NDCG@10:     {bm25_eval['ndcg'].mean():.4f}\")\n",
    "print(f\"Semantic Mean NDCG@10: {semantic_eval['ndcg'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Analyzing When Each Method Wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine evaluations\n",
    "comparison = bm25_eval.merge(semantic_eval, on=['query_id', 'query'], suffixes=('_bm25', '_semantic'))\n",
    "comparison['diff'] = comparison['ndcg_semantic'] - comparison['ndcg_bm25']\n",
    "\n",
    "print(f\"Semantic wins: {(comparison['diff'] > 0).sum()} queries\")\n",
    "print(f\"BM25 wins: {(comparison['diff'] < 0).sum()} queries\")\n",
    "print(f\"Tie: {(comparison['diff'] == 0).sum()} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries where semantic search wins big\n",
    "print(\"Queries where SEMANTIC wins:\")\n",
    "semantic_wins = comparison.nlargest(5, 'diff')\n",
    "semantic_wins[['query', 'ndcg_bm25', 'ndcg_semantic', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries where BM25 wins big\n",
    "print(\"Queries where BM25 wins:\")\n",
    "bm25_wins = comparison.nsmallest(5, 'diff')\n",
    "bm25_wins[['query', 'ndcg_bm25', 'ndcg_semantic', 'diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Key Takeaway\n",
    "\n",
    "**Similarity is NOT the same as relevance!**\n",
    "\n",
    "The embedding model learned general semantic similarity, but:\n",
    "- It wasn't trained on e-commerce product search\n",
    "- It doesn't know that product type is often more important than theme\n",
    "- It doesn't understand your specific business rules\n",
    "\n",
    "**Never assume embeddings will solve your search problem. Always evaluate with real relevance labels!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Hybrid Search: Best of Both Worlds\n",
    "\n",
    "Since BM25 and semantic search have different strengths, what if we **combine them**?\n",
    "\n",
    "## 6.1 Weighted Combination\n",
    "\n",
    "The simplest hybrid approach:\n",
    "1. Get BM25 scores (normalize to 0-1)\n",
    "2. Get semantic similarity scores (already 0-1)\n",
    "3. Combine: `hybrid = alpha * semantic + (1-alpha) * bm25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, sample_index, product_embeddings, products_df, \n",
    "                  sample_lengths, alpha=0.5, k=10):\n",
    "    \"\"\"\n",
    "    Combine BM25 and semantic search.\n",
    "    \n",
    "    alpha: weight for semantic (1-alpha for BM25)\n",
    "    \"\"\"\n",
    "    # Get BM25 scores\n",
    "    bm25_scores = score_bm25(query, sample_index, len(products_df), sample_lengths)\n",
    "    bm25_norm = normalize_scores(bm25_scores)\n",
    "    \n",
    "    # Get semantic scores\n",
    "    query_emb = get_embedding_local(query)\n",
    "    semantic_scores = batch_cosine_similarity(query_emb, product_embeddings)\n",
    "    # Semantic scores are already roughly 0-1, but let's normalize too\n",
    "    semantic_norm = normalize_scores(semantic_scores)\n",
    "    \n",
    "    # Combine\n",
    "    combined_scores = alpha * semantic_norm + (1 - alpha) * bm25_norm\n",
    "    \n",
    "    # Get top-k\n",
    "    top_k_idx = np.argsort(-combined_scores)[:k]\n",
    "    \n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results['hybrid_score'] = combined_scores[top_k_idx]\n",
    "    results['bm25_score'] = bm25_norm[top_k_idx]\n",
    "    results['semantic_score'] = semantic_norm[top_k_idx]\n",
    "    results['rank'] = range(1, k + 1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search\n",
    "query = \"star wars rug\"\n",
    "hybrid_results = hybrid_search(query, sample_index, product_embeddings, \n",
    "                               products_sample, sample_lengths, alpha=0.5)\n",
    "\n",
    "print(f\"Hybrid search for '{query}':\")\n",
    "hybrid_results[['rank', 'product_name', 'product_class', 'bm25_score', 'semantic_score', 'hybrid_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Finding the Optimal Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different alpha values\n",
    "alphas = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"Evaluating alpha={alpha}...\")\n",
    "    eval_df = evaluate_search(\n",
    "        lambda q: hybrid_search(q, sample_index, product_embeddings, \n",
    "                               products_sample, sample_lengths, alpha=alpha),\n",
    "        products_sample, sample_queries, sample_labels, k=10, verbose=False\n",
    "    )\n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'mean_ndcg': eval_df['ndcg'].mean()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results_df['alpha'], results_df['mean_ndcg'], 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Alpha (0=BM25 only, 1=Semantic only)', fontsize=12)\n",
    "plt.ylabel('Mean NDCG@10', fontsize=12)\n",
    "plt.title('Hybrid Search Performance vs Alpha', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_alpha = results_df.loc[results_df['mean_ndcg'].idxmax(), 'alpha']\n",
    "print(f\"\\nBest alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hybrid with best alpha\n",
    "print(f\"Evaluating Hybrid (alpha={best_alpha})...\")\n",
    "hybrid_eval = evaluate_search(\n",
    "    lambda q: hybrid_search(q, sample_index, product_embeddings, \n",
    "                           products_sample, sample_lengths, alpha=best_alpha),\n",
    "    products_sample, sample_queries, sample_labels, k=10\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"BM25 only:              {bm25_eval['ndcg'].mean():.4f}\")\n",
    "print(f\"Semantic only:          {semantic_eval['ndcg'].mean():.4f}\")\n",
    "print(f\"Hybrid (alpha={best_alpha}):     {hybrid_eval['ndcg'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid search often outperforms both individual methods!**\n",
    "\n",
    "This is because:\n",
    "- BM25 ensures exact keyword matches are found\n",
    "- Semantic adds synonym and concept matching\n",
    "- Together they cover each other's weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Challenges at Scale\n",
    "\n",
    "In production, semantic search faces several challenges:\n",
    "\n",
    "## 7.1 Computational Cost\n",
    "\n",
    "Comparing a query to **millions of products** requires millions of similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time semantic search at different scales\n",
    "query_emb = get_embedding_local(\"test query\")\n",
    "\n",
    "for n in [1000, 5000]:\n",
    "    subset = product_embeddings[:n]\n",
    "    start = time.time()\n",
    "    for _ in range(100):  # Run 100 times for stable measurement\n",
    "        _ = batch_cosine_similarity(query_emb, subset)\n",
    "    elapsed = (time.time() - start) / 100\n",
    "    print(f\"{n:,} products: {elapsed*1000:.2f}ms per query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions:**\n",
    "- **ANN (Approximate Nearest Neighbors)**: FAISS, HNSW, Pinecone\n",
    "- Trade exact results for ~100x speedup\n",
    "\n",
    "## 7.2 Other Challenges\n",
    "\n",
    "| Challenge | Problem | Solution |\n",
    "|-----------|---------|----------|\n",
    "| **Hubness** | Some vectors match everything | Normalize, diversity sampling |\n",
    "| **Filtering** | \"Red sofas under $500\" | Pre-filter then search |\n",
    "| **Staleness** | Products change, embeddings don't | Re-embed pipeline |\n",
    "| **Cold start** | New products have no signals | Use content-based embeddings |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Summary\n",
    "\n",
    "## What We Covered\n",
    "\n",
    "| Concept | What It Is | Key Insight |\n",
    "|---------|-----------|-------------|\n",
    "| **Embedding** | Dense vector representing meaning | Similar items = close vectors |\n",
    "| **Local vs API** | Hugging Face vs OpenAI | Trade-off: cost vs quality |\n",
    "| **Cosine Similarity** | Measures angle between vectors | Range -1 to 1, direction matters |\n",
    "| **Semantic Search** | Find by meaning, not keywords | Handles synonyms, paraphrases |\n",
    "| **Similarity != Relevance** | Training data != your domain | Always evaluate with real labels! |\n",
    "| **Hybrid Search** | BM25 + Semantic combined | Often beats either alone |\n",
    "\n",
    "## Can You Do These?\n",
    "\n",
    "- [ ] Get embeddings using both OpenAI API and local Hugging Face models\n",
    "- [ ] Calculate cosine similarity between vectors\n",
    "- [ ] Implement semantic search from scratch\n",
    "- [ ] Explain why similarity is not the same as relevance\n",
    "- [ ] Build hybrid search combining BM25 + embeddings\n",
    "- [ ] Evaluate search quality using NDCG\n",
    "- [ ] Choose between local and API embeddings based on requirements\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| Semantic search returns wrong product types | Consider hybrid search or filtering |\n",
    "| Embeddings are slow | Use local model for development, batch operations |\n",
    "| NDCG is low for semantic | Domain mismatch - consider fine-tuning |\n",
    "| Model download fails | Check internet connection, disk space |\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [Hugging Face Model Hub](https://huggingface.co/models)\n",
    "- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - Compare embedding models\n",
    "\n",
    "## Next Class\n",
    "\n",
    "We'll explore **RAG (Retrieval Augmented Generation)** - combining search with LLMs to build intelligent Q&A systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ce)",
   "language": "python",
   "name": "ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
