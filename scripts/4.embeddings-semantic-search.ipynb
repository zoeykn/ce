{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Embeddings & Semantic Search\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In this script we will explore **embeddings and semantic search**. These tools complement the lexical search tools that we covered last week. By the end of this session, you'll be able to:\n",
    "- Understand what embeddings are and how they encode meaning\n",
    "- Use both local (Hugging Face) and API-based (OpenAI) embedding models\n",
    "- Implement semantic search from scratch using cosine similarity\n",
    "- Discover why similarity does not equal relevance\n",
    "- Build hybrid search combining BM25 + embeddings\n",
    "- Compare search approaches using Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using modules in your code\n",
    "\n",
    "Starting today, we'll use a helpers module to organize reusable code. Instead of copying functions between notebooks, we will import them in our code like so:\n",
    "\n",
    "```python\n",
    "from helpers import load_wands_products, snowball_tokenize, score_bm25\n",
    "```\n",
    "\n",
    "This is similar to how you're using third-party libraries like pandas, numpy, matplotlib, etc. It is also how a good, professional codebase works: modularity helps us keep the code clean and easy to maintain.\n",
    "- If scripts use the same functions, we only need to fix a bug once, and then it is fixed everywhere\n",
    "- It allows us to have cleaner notebooks: we can focus on the lesson and spend our time rewriting boilerplate code.\n",
    "- It makes our code reusable: we can use the same functions work across lectures and homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# helpers imports\n",
    "from helpers import (\n",
    "    # Data loading\n",
    "    load_wands_products, load_wands_queries, load_wands_labels,\n",
    "    # BM25 \n",
    "    build_index, score_bm25, search_bm25,\n",
    "    # Evaluation\n",
    "    evaluate_search,\n",
    "    # Embeddings\n",
    "    get_local_model, batch_embed_local,\n",
    "    # Similarity\n",
    "    batch_cosine_similarity,\n",
    "    # Utility\n",
    "    normalize_scores\n",
    ")\n",
    "\n",
    "# Load environment variables for API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. From Keywords to Meaning\n",
    "\n",
    "## 1.1 Recap: BM25 and Lexical Search\n",
    "\n",
    "Last week, we built a search engine using **BM25** - a lexical search algorithm that:\n",
    "- Matches documents based on **exact token matches**\n",
    "- Uses **TF-IDF** scoring with saturation and length normalization\n",
    "- Gives you **precise control** over what matches\n",
    "\n",
    "Let's reload our WANDS data and BM25 index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WANDS dataset (same as Lecture 3 and Homework 3)\n",
    "products = load_wands_products()\n",
    "queries = load_wands_queries()\n",
    "labels = load_wands_labels()\n",
    "\n",
    "print(f\"Products: {len(products):,}\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(f\"Labels: {len(labels):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index on product names\n",
    "name_index, name_lengths = build_index(products['product_name'].tolist())\n",
    "print(f\"Index contains {len(name_index):,} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Limitations of Lexical Search\n",
    "\n",
    "BM25 is powerful, but it has a fundamental limitation: it only matches **exact tokens**.\n",
    "\n",
    "What happens when we search for synonyms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"couch\"\n",
    "couch_results = search_bm25(\"couch\", name_index, products, name_lengths, k=5)\n",
    "print(\"BM25 results for 'couch':\")\n",
    "couch_results[['product_name', 'bm25_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for \"sofa\" - a synonym!\n",
    "sofa_results = search_bm25(\"sofa\", name_index, products, name_lengths, k=5)\n",
    "print(\"BM25 results for 'sofa':\")\n",
    "sofa_results[['product_name', 'bm25_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for something that is not a product name but should match both sofas and couches\n",
    "relax_results = search_bm25(\"place to sit and relax\", name_index, products, name_lengths, k=5)\n",
    "print(\"BM25 results for 'place to sit and relax':\")\n",
    "relax_results[['product_name', 'bm25_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the problem:**\n",
    "- \"couch\" results contain products with \"couch\" in the name, \"sofa\" results contain products with \"sofa\" in the name, but they are **synonyms** - a user searching for \"couch\" would probably want sofas too!\n",
    "- \"Place to sit and relax\" is a conceptual query that should match both sofas and couches, and the results we get back are not relevant at all.\n",
    "\n",
    "BM25 treats them as completely different words because it only matches exact tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. What are embeddings?\n",
    "\n",
    "> **TERM: Embedding**  \n",
    "> A **dense vector representation** that maps text (or other data) to a point in high-dimensional space where **semantically similar items are close together**.\n",
    "\n",
    "Think of it as assigning \"coordinates\" to the **meaning** of text:\n",
    "- \"couch\" and \"sofa\" would have similar coordinates (close together)\n",
    "- \"couch\" and \"refrigerator\" would have different coordinates (far apart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Getting Your First Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Get an embedding using OpenAI's API via LiteLLM\n",
    "# This is what happens inside get_embedding_openai()\n",
    "response = litellm.embedding(model=\"text-embedding-3-small\", input=[\"couch\"])\n",
    "couch_emb = np.array(response.data[0][\"embedding\"])\n",
    "\n",
    "print(f\"Type: {type(couch_emb)}\")\n",
    "print(f\"Dimension: {len(couch_emb)}\")\n",
    "print(f\"First 10 values: {couch_emb[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is a **1536-dimensional vector** of floating-point numbers.\n",
    "\n",
    "Each dimension captures some aspect of the word's meaning - but unlike features we design ourselves, these are **latent features** learned by the model.\n",
    "\n",
    "> **TERM: Latent Features**  \n",
    "> Hidden dimensions in the embedding that capture abstract concepts. They're not directly interpretable like \"color=red\" or \"size=large\" - they're patterns the model discovered during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Measuring Similarity with Cosine\n",
    "\n",
    "To find similar items, we measure the \"distance\" between embeddings using **cosine similarity**:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\times \\|b\\|}$$\n",
    "\n",
    "- **1.0** = identical direction (most similar)\n",
    "- **0.0** = perpendicular (unrelated)\n",
    "- **-1.0** = opposite direction (most dissimilar)\n",
    "\n",
    "Why cosine? It focuses on **direction** (meaning) not **magnitude** (length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Embeddings Capture Meaning\n",
    "\n",
    "Let's see how embeddings capture the relationship between words. We'll compute cosine similarity for each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for related words\n",
    "words = [\"couch\", \"sofa\", \"chair\", \"table\", \"refrigerator\"]\n",
    "\n",
    "# Get embeddings using OpenAI API via LiteLLM\n",
    "embeddings = {}\n",
    "for word in words:\n",
    "    response = litellm.embedding(model=\"text-embedding-3-large\", input=[word])\n",
    "    embeddings[word] = np.array(response.data[0][\"embedding\"])\n",
    "\n",
    "# Calculate similarity between all pairs using cosine similarity\n",
    "print(\"Similarity matrix:\")\n",
    "\n",
    "word_width = max(len(w) for w in words) + 2\n",
    "header = \"\".join([f\"{'':<{word_width}s}\"] + [f\"{w:>{word_width}s}\" for w in words])\n",
    "print(header)\n",
    "print(\"-\" * (word_width + len(words) * word_width))\n",
    "\n",
    "for w1 in words:\n",
    "    row = [f\"{w1:<{word_width}s}\"]\n",
    "    for w2 in words:\n",
    "        # Cosine similarity: dot product divided by product of norms\n",
    "        sim = np.dot(embeddings[w1], embeddings[w2]) / (np.linalg.norm(embeddings[w1]) * np.linalg.norm(embeddings[w2]))\n",
    "        row.append(f\"{sim:>{word_width}.2f}\")\n",
    "    print(\"\".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice?**\n",
    "- \"couch\" and \"sofa\" have **very high similarity** (~0.75) - the model knows they're semantically similar\n",
    "- Furniture items (couch, sofa, chair, table) are more similar to each other\n",
    "- \"refrigerator\" is less similar to the furniture items\n",
    "\n",
    "The embedding model learned these relationships from training on massive amounts of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for related words\n",
    "words_2 = [\"Apostolos Filippas\", \"Tilda Swinton\", \"Technology\", \"Movies\", \"Suspiria\", \"Greek\", \"British\"]\n",
    "\n",
    "# Get embeddings using OpenAI API via LiteLLM\n",
    "embeddings_2 = {}\n",
    "for word in words_2:\n",
    "    response = litellm.embedding(model=\"text-embedding-3-large\", input=[word])\n",
    "    embeddings_2[word] = np.array(response.data[0][\"embedding\"])\n",
    "\n",
    "# Calculate similarity between all pairs\n",
    "print(\"Similarity matrix:\")\n",
    "\n",
    "word_width = max(len(w) for w in words_2) + 2\n",
    "header = \"\".join([f\"{'':<{word_width}s}\"] + [f\"{w:>{word_width}s}\" for w in words_2])\n",
    "print(header)\n",
    "print(\"-\" * (word_width + len(words_2) * word_width))\n",
    "\n",
    "for w1 in words_2:\n",
    "    row = [f\"{w1:<{word_width}s}\"]\n",
    "    for w2 in words_2:\n",
    "        # Inline cosine similarity\n",
    "        sim = np.dot(embeddings_2[w1], embeddings_2[w2]) / (np.linalg.norm(embeddings_2[w1]) * np.linalg.norm(embeddings_2[w2]))\n",
    "        row.append(f\"{sim:>{word_width}.2f}\")\n",
    "    print(\"\".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2.5 Local vs API Embeddings\n",
    "\n",
    "> **TERM: Hugging Face**  \n",
    "> An open-source platform hosting thousands of pre-trained AI models. Think of it as \"GitHub for AI models\" - you can download and run models locally without API calls or costs.\n",
    "\n",
    "So far we've used OpenAI's embedding API. But there's another option: **run models locally**!\n",
    "\n",
    "## 2.5.1 Loading a Local Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a local embedding - first call downloads the model (~80MB)\n",
    "model = get_local_model(\"all-MiniLM-L6-v2\")\n",
    "local_emb = model.encode(\"wooden coffee table\", convert_to_numpy=True)\n",
    "\n",
    "print(f\"Local embedding dimension: {len(local_emb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dimensions\n",
    "api_response = litellm.embedding(model=\"text-embedding-3-small\", input=[\"wooden coffee table\"])\n",
    "api_emb = np.array(api_response.data[0][\"embedding\"])\n",
    "\n",
    "print(f\"OpenAI (API): {len(api_emb)} dimensions\")\n",
    "print(f\"MiniLM (Local): {len(local_emb)} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.3 Trade-offs: API vs Local\n",
    "\n",
    "| Aspect | API (OpenAI) | Local (Hugging Face) |\n",
    "|--------|-------------|---------------------|\n",
    "| **Cost** | ~$0.02 per 1M tokens | FREE |\n",
    "| **Dimensions** | 1536 (more expressive) | 384 (more compact) |\n",
    "| **Quality** | Generally higher | Good for most tasks |\n",
    "| **Speed** | Network latency | Faster for batches |\n",
    "| **Privacy** | Data sent to API | Data stays local |\n",
    "| **Setup** | Just API key | Downloads model (~80MB) |\n",
    "\n",
    "**When to use which?**\n",
    "- **Prototyping/Learning**: Local - free experimentation!\n",
    "- **Production with privacy needs**: Local\n",
    "- **Production needing best quality**: API\n",
    "- **High volume, cost-sensitive**: Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Measuring Similarity with Cosine\n",
    "\n",
    "## 3.1 Why Cosine Similarity?\n",
    "\n",
    "To find similar items, we need to measure the \"distance\" between embeddings. **Cosine similarity** measures the angle between two vectors:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\times \\|b\\|}$$\n",
    "\n",
    "- **1.0** = identical direction (most similar)\n",
    "- **0.0** = perpendicular (unrelated)\n",
    "- **-1.0** = opposite direction (most dissimilar)\n",
    "\n",
    "Why cosine instead of Euclidean distance? Cosine focuses on **direction** (meaning) not **magnitude** (length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cosine similarity formula implemented manually:\n",
    "def cosine_similarity_manual(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Verify it works\n",
    "sim = cosine_similarity_manual(embeddings[\"couch\"], embeddings[\"sofa\"])\n",
    "print(f\"Cosine similarity (couch, sofa): {sim:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Batch Similarity for Efficiency\n",
    "\n",
    "When searching, we need to compare one query against **thousands of products**. Doing this one-by-one is slow. Instead, we use **matrix operations**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all word embeddings into a matrix\n",
    "word_matrix = np.array([embeddings[w] for w in words])\n",
    "print(f\"Matrix shape: {word_matrix.shape}\")\n",
    "\n",
    "# Query embedding\n",
    "query_emb = embeddings[\"couch\"]\n",
    "\n",
    "# Calculate similarity to all words at once\n",
    "similarities = batch_cosine_similarity(query_emb, word_matrix)\n",
    "\n",
    "for word, sim in zip(words, similarities):\n",
    "    print(f\"{word:15s}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Building Semantic Search from Scratch\n",
    "\n",
    "Now let's build a working semantic search engine!\n",
    "\n",
    "## 4.1 The Semantic Search Pipeline\n",
    "\n",
    "1. **Embed all products** (offline, once)\n",
    "2. **Embed the query** (at search time)\n",
    "3. **Calculate similarity** between query and all products\n",
    "4. **Return top-k** most similar products\n",
    "\n",
    "## 4.2 Embedding Products\n",
    "\n",
    "For speed in class, we'll work with a sample of 10,000 products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get consistent sample (same for everyone)\n",
    "products_sample = products.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "print(f\"Working with {len(products_sample):,} products\")\n",
    "products_sample[['product_id', 'product_name', 'product_class']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text for embedding: combine name and class\n",
    "products_sample['embed_text'] = (\n",
    "    products_sample['product_name'].fillna('') + ' ' +\n",
    "    products_sample['product_class'].fillna('')\n",
    ")\n",
    "\n",
    "products_sample['embed_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all products using local model\n",
    "# This took me 3.5 seconds\n",
    "print(\"Embedding products...\")\n",
    "start = time.time()\n",
    "product_embeddings = batch_embed_local(\n",
    "    products_sample['embed_text'].tolist(),\n",
    "    show_progress=True\n",
    ")\n",
    "print(f\"Done in {time.time() - start:.1f}s\")\n",
    "print(f\"Embeddings shape: {product_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings so we don't have to recompute\n",
    "np.save('temp/product_embeddings_sample.npy', product_embeddings)\n",
    "products_sample.to_csv('temp/products_sample.csv', index=False)\n",
    "print(\"Saved embeddings and sample to 'scripts/temp/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Implementing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_local(query, product_embeddings, products_df, k=10):\n",
    "    \"\"\"Search products using local embedding similarity.\"\"\"\n",
    "    # 1. Embed the query\n",
    "    model = get_local_model(\"all-MiniLM-L6-v2\")\n",
    "    query_emb = model.encode(query, convert_to_numpy=True)\n",
    "    \n",
    "    # 2. Calculate similarity to all products\n",
    "    similarities = batch_cosine_similarity(query_emb, product_embeddings)\n",
    "    \n",
    "    # 3. Get top-k indices\n",
    "    top_k_idx = np.argsort(-similarities)[:k]\n",
    "    \n",
    "    # 4. Build results DataFrame\n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results['similarity'] = similarities[top_k_idx]\n",
    "    results['rank'] = range(1, k + 1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search!\n",
    "results = semantic_search_local(\"couch\", product_embeddings, products_sample)\n",
    "results[['rank', 'product_name', 'product_class', 'similarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the synonym problem that BM25 couldn't solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index for the sample\n",
    "sample_index, sample_lengths = build_index(products_sample['product_name'].tolist())\n",
    "\n",
    "# Search for \"sofa\" with BM25\n",
    "bm25_results = search_bm25(\"sofa\", sample_index, products_sample, sample_lengths, k=10)\n",
    "print(\"BM25 for 'sofa':\")\n",
    "print(bm25_results[['product_name', 'bm25_score']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Search for \"sofa\" with semantic search\n",
    "sem_results = semantic_search_local(\"sofa\", product_embeddings, products_sample, k=10)\n",
    "print(\"Semantic for 'sofa':\")\n",
    "print(sem_results[['product_name', 'similarity']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic search finds both \"sofa\" AND \"couch\" products!** It understands they're related concepts.\n",
    "\n",
    "Let's try another query that BM25 struggles with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A conceptual query - no exact keyword match\n",
    "query = \"place to sit and relax\"\n",
    "\n",
    "bm25_results = search_bm25(query, sample_index, products_sample, sample_lengths, k=5)\n",
    "print(f\"BM25 for '{query}':\")\n",
    "print(bm25_results[['product_name', 'bm25_score']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "sem_results = semantic_search_local(query, product_embeddings, products_sample, k=5)\n",
    "print(f\"Semantic for '{query}':\")\n",
    "print(sem_results[['product_name', 'similarity']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Comparing lexical and semantic search\n",
    "\n",
    "Semantic search seems magical - it finds synonyms and understands concepts! But it doesn't always work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's quantify how well each search method performs using **Recall@k**:\n",
    "\n",
    "> **Recall@k** = What fraction of relevant items did we find in the top k results?\n",
    ">\n",
    "> Example: If there are 20 relevant products and we found 3 of them in our top 10, Recall@10 = 3/20 = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter queries to those with products in our sample\n",
    "sample_product_ids = set(products_sample['product_id'])\n",
    "sample_labels = labels[labels['product_id'].isin(sample_product_ids)]\n",
    "sample_query_ids = set(sample_labels['query_id'])\n",
    "sample_queries = queries[queries['query_id'].isin(sample_query_ids)]\n",
    "\n",
    "print(f\"Queries with products in sample: {len(sample_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BM25 on sample\n",
    "print(\"Evaluating BM25...\")\n",
    "bm25_eval = evaluate_search(\n",
    "    lambda q: search_bm25(q, sample_index, products_sample, sample_lengths, k=10),\n",
    "    sample_queries, sample_labels, k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Semantic Search on sample\n",
    "print(\"Evaluating Semantic Search...\")\n",
    "semantic_eval = evaluate_search(\n",
    "    lambda q: semantic_search_local(q, product_embeddings, products_sample, k=10),\n",
    "    sample_queries, sample_labels, k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare!\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "print(f\"BM25 Mean Recall@10:     {bm25_eval['recall'].mean():.4f}\")\n",
    "print(f\"Semantic Mean Recall@10: {semantic_eval['recall'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see when each method wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine evaluations\n",
    "comparison = bm25_eval.merge(semantic_eval, on=['query_id', 'query'], suffixes=('_bm25', '_semantic'))\n",
    "comparison['diff'] = comparison['recall_semantic'] - comparison['recall_bm25']\n",
    "\n",
    "print(f\"Semantic wins: {(comparison['diff'] > 0).sum()} queries\")\n",
    "print(f\"BM25 wins: {(comparison['diff'] < 0).sum()} queries\")\n",
    "print(f\"Tie: {(comparison['diff'] == 0).sum()} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries where semantic search wins big\n",
    "print(\"Queries where SEMANTIC wins:\")\n",
    "semantic_wins = comparison.nlargest(5, 'diff')\n",
    "semantic_wins[['query', 'recall_bm25', 'recall_semantic', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries where BM25 wins big\n",
    "print(\"Queries where BM25 wins:\")\n",
    "bm25_wins = comparison.nsmallest(5, 'diff')\n",
    "bm25_wins[['query', 'recall_bm25', 'recall_semantic', 'diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Key Takeaway\n",
    "\n",
    "**Similarity is NOT the same as relevance!**\n",
    "\n",
    "The embedding model learned general semantic similarity, but:\n",
    "- It wasn't trained on e-commerce product search\n",
    "- It doesn't know whether product type is often more important than theme\n",
    "- It doesn't understand your specific business rules\n",
    "\n",
    "**Never assume embeddings will solve your search problem. Always evaluate with real relevance labels!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Hybrid Search: Best of Both Worlds\n",
    "\n",
    "Since BM25 and semantic search have different strengths, what if we **combine them**?\n",
    "\n",
    "## 6.1 Weighted Combination\n",
    "\n",
    "The simplest hybrid approach:\n",
    "1. Get BM25 scores (normalize to 0-1)\n",
    "2. Get semantic similarity scores (already 0-1)\n",
    "3. Combine: `hybrid = alpha * semantic + (1-alpha) * bm25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, sample_index, product_embeddings, products_df, \n",
    "                  sample_lengths, alpha=0.5, k=10):\n",
    "    \"\"\"\n",
    "    Combine BM25 and semantic search.\n",
    "    \n",
    "    alpha: weight for semantic (1-alpha for BM25)\n",
    "    \"\"\"\n",
    "    # Get BM25 scores\n",
    "    bm25_scores = score_bm25(query, sample_index, len(products_df), sample_lengths)\n",
    "    bm25_norm = normalize_scores(bm25_scores)\n",
    "    \n",
    "    # Get semantic scores\n",
    "    model = get_local_model(\"all-MiniLM-L6-v2\")\n",
    "    query_emb = model.encode(query, convert_to_numpy=True)\n",
    "    semantic_scores = batch_cosine_similarity(query_emb, product_embeddings)\n",
    "    # Semantic scores are already roughly 0-1, but let's normalize too\n",
    "    semantic_norm = normalize_scores(semantic_scores)\n",
    "    \n",
    "    # Combine\n",
    "    combined_scores = alpha * semantic_norm + (1 - alpha) * bm25_norm\n",
    "    \n",
    "    # Get top-k\n",
    "    top_k_idx = np.argsort(-combined_scores)[:k]\n",
    "    \n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results['hybrid_score'] = combined_scores[top_k_idx]\n",
    "    results['bm25_score'] = bm25_norm[top_k_idx]\n",
    "    results['semantic_score'] = semantic_norm[top_k_idx]\n",
    "    results['rank'] = range(1, k + 1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search\n",
    "query = \"star wars rug\"\n",
    "hybrid_results = hybrid_search(query, sample_index, product_embeddings, \n",
    "                               products_sample, sample_lengths, alpha=0.5)\n",
    "\n",
    "print(f\"Hybrid search for '{query}':\")\n",
    "hybrid_results[['rank', 'product_name', 'product_class', 'bm25_score', 'semantic_score', 'hybrid_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Finding the Optimal Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different alpha values\n",
    "alphas = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"Evaluating alpha={alpha}...\")\n",
    "    eval_df = evaluate_search(\n",
    "        lambda q, a=alpha: hybrid_search(q, sample_index, product_embeddings, \n",
    "                               products_sample, sample_lengths, alpha=a),\n",
    "        sample_queries, sample_labels, k=10, verbose=False\n",
    "    )\n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'mean_recall': eval_df['recall'].mean()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results_df['alpha'], results_df['mean_recall'], 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Alpha (0=BM25 only, 1=Semantic only)', fontsize=12)\n",
    "plt.ylabel('Mean Recall@10', fontsize=12)\n",
    "plt.title('Hybrid Search Performance vs Alpha', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_alpha = results_df.loc[results_df['mean_recall'].idxmax(), 'alpha']\n",
    "print(f\"\\nBest alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hybrid with best alpha\n",
    "print(f\"Evaluating Hybrid (alpha={best_alpha})...\")\n",
    "hybrid_eval = evaluate_search(\n",
    "    lambda q: hybrid_search(q, sample_index, product_embeddings, \n",
    "                           products_sample, sample_lengths, alpha=best_alpha),\n",
    "    sample_queries, sample_labels, k=10\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"BM25 only:              {bm25_eval['recall'].mean():.4f}\")\n",
    "print(f\"Semantic only:          {semantic_eval['recall'].mean():.4f}\")\n",
    "print(f\"Hybrid (alpha={best_alpha}):     {hybrid_eval['recall'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid search often outperforms both individual methods!**\n",
    "\n",
    "This is because:\n",
    "- BM25 ensures exact keyword matches are found\n",
    "- Semantic adds synonym and concept matching\n",
    "- Together they cover each other's weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Covered\n",
    "\n",
    "| Concept | What It Is | Key Insight |\n",
    "|---------|-----------|-------------|\n",
    "| **Embedding** | Dense vector representing meaning | Similar items = close vectors |\n",
    "| **Local vs API** | Hugging Face vs OpenAI | Trade-off: cost vs quality |\n",
    "| **Cosine Similarity** | Measures angle between vectors | Range -1 to 1, direction matters |\n",
    "| **Semantic Search** | Find by meaning, not keywords | Handles synonyms, paraphrases |\n",
    "| **Similarity != Relevance** | Training data != your domain | Always evaluate with real labels! |\n",
    "| **Hybrid Search** | BM25 + Semantic combined | Often beats either alone |\n",
    "\n",
    "## Can You Do These?\n",
    "\n",
    "- [ ] Get embeddings using both OpenAI API and local Hugging Face models\n",
    "- [ ] Calculate cosine similarity between vectors\n",
    "- [ ] Implement semantic search from scratch\n",
    "- [ ] Explain why similarity is not the same as relevance\n",
    "- [ ] Build hybrid search combining BM25 + embeddings\n",
    "- [ ] Evaluate search quality using Recall\n",
    "- [ ] Choose between local and API embeddings based on requirements\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| Semantic search returns wrong product types | Consider hybrid search or filtering |\n",
    "| Embeddings are slow | Use local model for development, batch operations |\n",
    "| Recall is low for semantic | Domain mismatch - consider fine-tuning |\n",
    "| Model download fails | Check internet connection, disk space |\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [Hugging Face Model Hub](https://huggingface.co/models)\n",
    "- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - Compare embedding models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ce)",
   "language": "python",
   "name": "ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
