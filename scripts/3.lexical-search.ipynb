{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Lexical Search & BM25\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In this lecture, we'll explore **lexical search** â€” the foundation of many search engines like Elasticsearch and Vespa, but also an amazing tool for context engineering -- and an amazing tool in the hands of AI Agents. \n",
    "\n",
    "Don't worry if this seems like a lot â€” we'll take it step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # you know what pandas is -- it's a data analysis library\n",
    "import numpy as np # a numerical computing library\n",
    "from collections import Counter # a dict that counts the number of times each element appears in a list\n",
    "from typing import Callable # a type hint for functions\n",
    "from string import punctuation # a string of all the punctuation characters\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with counter \n",
    "Counter([1,2,3,4,5,6,7,8,9,10,10,10,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Introduction to Lexical Search\n",
    "\n",
    "## 1.1 What is Lexical Search?\n",
    "\n",
    "**Lexical search** finds documents by matching the exact words (tokens) in your query against words in your documents.\n",
    "\n",
    "> ðŸ“š **TERM: Lexical Search**  \n",
    "> A search method that matches documents based on exact string/token matches. \"Lexical\" refers to words or vocabulary â€” the search operates on the literal text.\n",
    "\n",
    "Think of it like using `Ctrl+F` in a document, but smarter:\n",
    "- It can match across many documents\n",
    "- It ranks results by relevance\n",
    "- You control exactly what counts as a \"match\"\n",
    "\n",
    "## 1.2 Lexical vs. Embedding-Based Search (next week)\n",
    "\n",
    "| Aspect | Lexical Search | Embedding Search |\n",
    "|--------|---------------|------------------|\n",
    "| **How it works** | Exact token matching | Semantic similarity |\n",
    "| **Control** | High â€” you decide what matches | Low â€” model decides |\n",
    "| **Speed** | Very fast (inverted index) | Slower (vector math) |\n",
    "| **Best for** | Precise queries, keywords, IDs | Fuzzy queries, synonyms |\n",
    "\n",
    "**Key insight**: Embedding search is a **sledgehammer** â€” powerful but imprecise. Lexical search is a **scalpel** â€” precise and controllable.\n",
    "\n",
    "## 1.3 The Inverted Index\n",
    "\n",
    "> ðŸ“š **TERM: Inverted Index**  \n",
    "> A data structure that maps each unique term to the list of documents containing it. This enables fast lookups: instead of scanning all documents, you look up which documents contain your search term.\n",
    "\n",
    "```python\n",
    "Regular index:  Doc1 â†’ [\"hello\", \"world\"]\n",
    "                Doc2 â†’ [\"hello\", \"there\"]\n",
    "\n",
    "Inverted index: \"hello\" â†’ [Doc1, Doc2]\n",
    "                \"world\" â†’ [Doc1]\n",
    "                \"there\" â†’ [Doc2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Tokenization\n",
    "\n",
    "Before we can search, we need to break text into searchable units called **tokens**.\n",
    "\n",
    "## 2.1 Index-Time Tokenization\n",
    "\n",
    "Let's start with some sample data â€” a chat transcript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample chat data\n",
    "chat_transcript = [\n",
    "    \"Hi this is Apostolos, I'd like to complain about the weather\",\n",
    "    \"Apostolos, this is Tom, support for Earth's Climate, how can we help?\",\n",
    "    \"Tom, can I speak to your manager?\",\n",
    "    \"Hi, this is Sue, Tom's boss. What can I do for you?\",\n",
    "    \"I'd like to complain about the ski conditions in Greece\",\n",
    "    \"Oh apostolos thats terrible, lets see what we can do.\"\n",
    "]\n",
    "\n",
    "msgs = pd.DataFrame({\n",
    "    \"name\": [\"Apostolos\", \"Tom\", \"Apostolos\", \"Sue\", \"Apostolos\", \"Sue\"],\n",
    "    \"msg\": chat_transcript\n",
    "})\n",
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Whitespace Tokenization\n",
    "\n",
    "The simplest tokenizer just splits on whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Split text on whitespace.\"\"\"\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokenize(\"Mary had a little lamb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem: \"Apostolos,\" != \"apostolos\"\n",
    "\n",
    "Let's see what happens when we tokenize our messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how \"Apostolos\" appears in different messages\n",
    "print(\"Message 0:\", whitespace_tokenize(chat_transcript[0]))\n",
    "print(\"Message 5:\", whitespace_tokenize(chat_transcript[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the problem:\n",
    "- `\"Apostolos,\"` (with comma) â‰  `\"apostolos\"` (lowercase)\n",
    "- These are **different strings**, so they won't match!\n",
    "\n",
    "**Takeaway**: Lexical search is about _extremely precise control of string matching_. YOU decide what to accept as equivalent words.\n",
    "\n",
    "### Better Tokenization\n",
    "\n",
    "Let's improve our tokenizer:\n",
    "1. Lowercase everything\n",
    "2. Remove punctuation\n",
    "3. Split on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Tokenize with lowercase and punctuation removal.\"\"\"\n",
    "    lowercased = text.lower()\n",
    "    without_punctuation = lowercased.translate(str.maketrans('', '', punctuation))\n",
    "    return without_punctuation.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "print(\"Before:\", whitespace_tokenize(\"Apostolos, that weirdo?\"))\n",
    "print(\"After: \", better_tokenize(\"Apostolos, that weirdo?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `\"Apostolos,\"` and `\"apostolos\"` both become `\"apostolos\"`!\n",
    "\n",
    "### Building an Inverted Index from Scratch\n",
    "\n",
    "Let's build our own inverted index using just Python dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(docs: list[str], tokenizer: Callable[[str], list[str]]) -> tuple[dict[str, dict[int, int]], list[int]]:\n",
    "    \"\"\"\n",
    "    Build an inverted index from a list of documents.\n",
    "    \n",
    "    Returns:\n",
    "        index: dict mapping term -> {doc_id: term_count}\n",
    "        doc_lengths: list of document lengths (in tokens)\n",
    "    \"\"\"\n",
    "    index = {}  # term -> {doc_id: count}\n",
    "    doc_lengths = []\n",
    "    \n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        tokens = tokenizer(doc)\n",
    "        doc_lengths.append(len(tokens))\n",
    "        \n",
    "        # Count term frequencies in this document\n",
    "        term_counts = Counter(tokens)\n",
    "        \n",
    "        for term, count in term_counts.items():\n",
    "            if term not in index:\n",
    "                index[term] = {}\n",
    "            index[term][doc_id] = count\n",
    "    \n",
    "    return index, doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the index\n",
    "index, doc_lengths = build_index(msgs['msg'].tolist(), better_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the index entry for \"apostolos\"\n",
    "print(\"Documents containing 'apostolos':\")\n",
    "print(index.get('apostolos', {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Query-Time Tokenization\n",
    "\n",
    "Now let's search! We need to tokenize the query the same way we tokenized the documents.\n",
    "\n",
    "### Simple Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, index: dict[str, dict[int, int]], num_docs: int, tokenizer: Callable[[str], list[str]]) -> set[int]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the query.\n",
    "    Returns set of matching document IDs.\n",
    "    \"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    matching_docs = set()\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in index:\n",
    "            matching_docs.update(index[token].keys())\n",
    "    \n",
    "    return matching_docs\n",
    "\n",
    "# Search for \"apostolos\"\n",
    "results = search(\"apostolos\", index, len(msgs), better_tokenize)\n",
    "print(f\"Documents matching 'apostolos': {results}\")\n",
    "msgs.iloc[list(results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Queries vs AND Queries\n",
    "\n",
    "What if we search for multiple terms like \"apostolos complaint\"?\n",
    "\n",
    "- **OR query**: Match documents containing ANY of the terms\n",
    "- **AND query**: Match documents containing ALL of the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_or(query, index, num_docs, tokenizer):\n",
    "    \"\"\"OR query: match ANY term.\"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    matches = np.zeros(num_docs, dtype=bool)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in index:\n",
    "            for doc_id in index[token]:\n",
    "                matches[doc_id] = True  # OR: set to True if any term matches\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def search_and(query, index, num_docs, tokenizer):\n",
    "    \"\"\"AND query: match ALL terms.\"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    matches = np.ones(num_docs, dtype=bool)  # Start with all True\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        token_matches = np.zeros(num_docs, dtype=bool)\n",
    "        if token in index:\n",
    "            for doc_id in index[token]:\n",
    "                token_matches[doc_id] = True\n",
    "        matches &= token_matches  # AND: must match all terms\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR query: \"apostolos\" OR \"complaint\"\n",
    "or_matches = search_or(\"apostolos complain\", index, len(msgs), better_tokenize)\n",
    "print(\"OR query results (apostolos OR complain):\")\n",
    "msgs[or_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND query: \"apostolos\" AND \"complaint\"\n",
    "and_matches = search_and(\"apostolos complain\", index, len(msgs), better_tokenize)\n",
    "print(\"AND query results (apostolos AND complain):\")\n",
    "msgs[and_matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ðŸ›‘ **TRY IT NOW**\n",
    "1. Search for \"tom manager\" with OR query â€” how many results?\n",
    "2. Search for \"tom manager\" with AND query â€” how many results?\n",
    "3. What happens if you search for a word that doesn't exist?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for \"tom manager\" with OR query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for \"tom manager\" with AND query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Scoring with TF*IDF\n",
    "\n",
    "So far we've only checked if documents match. But how do we **rank** them by relevance?\n",
    "\n",
    "## 3.1 Term Frequency (TF)\n",
    "\n",
    "> ðŸ“š **TERM: Term Frequency (TF)**  \n",
    "> The number of times a term appears in a document. More occurrences = more relevant (to a point).\n",
    "\n",
    "Let's build helper functions to get term statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(term, doc_id, index):\n",
    "    \"\"\"Get term frequency for a term in a document.\"\"\"\n",
    "    if term in index and doc_id in index[term]:\n",
    "        return index[term][doc_id]\n",
    "    return 0\n",
    "\n",
    "def get_df(term, index):\n",
    "    \"\"\"Get document frequency for a term (how many docs contain it).\"\"\"\n",
    "    if term in index:\n",
    "        return len(index[term])\n",
    "    return 0\n",
    "\n",
    "# Test it\n",
    "print(f\"TF of 'apostolos' in doc 0: {get_tf('apostolos', 0, index)}\")\n",
    "print(f\"TF of 'apostolos' in doc 5: {get_tf('apostolos', 5, index)}\")\n",
    "print(f\"DF of 'apostolos': {get_df('apostolos', index)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Document Frequency (DF) and IDF\n",
    "\n",
    "> ðŸ“š **TERM: Inverse Document Frequency (IDF)**  \n",
    "> A measure of how rare/specific a term is. IDF = 1 / DF (approximately). Rare terms have high IDF; common terms have low IDF.\n",
    "\n",
    "**Why IDF matters**: If a term appears in every document, it's not useful for distinguishing between them. Rare terms are more informative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare common vs rare terms\n",
    "print(\"Common terms (appear in many docs):\")\n",
    "print(f\"  'this' appears in {get_df('this', index)} docs\")\n",
    "print(f\"  'can' appears in {get_df('can', index)} docs\")\n",
    "\n",
    "print(\"\\nRare terms (appear in few docs):\")\n",
    "print(f\"  'virginia' appears in {get_df('virginia', index)} docs\")\n",
    "print(f\"  'manager' appears in {get_df('manager', index)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 TF*IDF Scoring\n",
    "\n",
    "> ðŸ“š **TERM: TF*IDF**  \n",
    "> A scoring formula that multiplies Term Frequency by Inverse Document Frequency. Documents score high when they contain the search term frequently AND the term is rare across documents.\n",
    "\n",
    "Let's implement TF*IDF scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tfidf(query, index, num_docs, doc_lengths, tokenizer):\n",
    "    \"\"\"\n",
    "    Score documents using TF*IDF.\n",
    "    Returns array of scores for each document.\n",
    "    \"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    scores = np.zeros(num_docs)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        df = get_df(token, index)\n",
    "        if df == 0:\n",
    "            continue\n",
    "        \n",
    "        idf = 1.0 / df  # Simple IDF\n",
    "        \n",
    "        for doc_id in range(num_docs):\n",
    "            tf = get_tf(token, doc_id, index)\n",
    "            scores[doc_id] += tf * idf\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score documents for \"apostolos complain\"\n",
    "scores = score_tfidf(\"apostolos complain\", index, len(msgs), doc_lengths, better_tokenize)\n",
    "\n",
    "msgs_with_scores = msgs.copy()\n",
    "msgs_with_scores['score'] = scores\n",
    "msgs_with_scores.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Rare Terms Score Higher\n",
    "\n",
    "Let's see the individual term contributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down scoring by term\n",
    "query = \"apostolos complain\"\n",
    "query_tokens = better_tokenize(query)\n",
    "\n",
    "for token in query_tokens:\n",
    "    df = get_df(token, index)\n",
    "    idf = 1.0 / df if df > 0 else 0\n",
    "    print(f\"Term '{token}': DF={df}, IDF={idf:.3f}\")\n",
    "    \n",
    "    # Show which docs have this term\n",
    "    if token in index:\n",
    "        for doc_id, tf in index[token].items():\n",
    "            print(f\"  Doc {doc_id}: TF={tf}, score contribution={tf * idf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Multi-Field Search\n",
    "\n",
    "Real documents often have multiple fields (title, body, tags). Let's search across multiple fields!\n",
    "\n",
    "## 4.1 Indexing Multiple Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build separate indices for message and name fields\n",
    "msg_index, msg_lengths = build_index(msgs['msg'].tolist(), better_tokenize)\n",
    "name_index, name_lengths = build_index(msgs['name'].tolist(), better_tokenize)\n",
    "\n",
    "print(f\"Message index has {len(msg_index)} unique terms\")\n",
    "print(f\"Name index has {len(name_index)} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Stemming for Normalization\n",
    "\n",
    "> ðŸ“š **TERM: Stemming**  \n",
    "> Reducing words to their root form. For example: \"complaint\", \"complaining\", \"complained\" all become \"complain\". This helps match different forms of the same word.\n",
    "\n",
    "Let's add a simple stemmer to our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple suffix-stripping stemmer (for demonstration)\n",
    "def simple_stem(word):\n",
    "    \"\"\"Very basic stemmer - removes common suffixes.\"\"\"\n",
    "    suffixes = ['ing', 'ed', 'er', 'est', 's', 'ly']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "def stemming_tokenize(text):\n",
    "    \"\"\"Tokenize with stemming.\"\"\"\n",
    "    tokens = better_tokenize(text)\n",
    "    return [simple_stem(token) for token in tokens]\n",
    "\n",
    "# Test stemming\n",
    "print(\"Without stemming:\", better_tokenize(\"I have complaints about complaining\"))\n",
    "print(\"With stemming:   \", stemming_tokenize(\"I have complaints about complaining\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild indices with stemming\n",
    "msg_index_stem, msg_lengths = build_index(msgs['msg'].tolist(), stemming_tokenize)\n",
    "name_index_stem, name_lengths = build_index(msgs['name'].tolist(), stemming_tokenize)\n",
    "\n",
    "# Now \"complaint\" and \"complain\" map to the same token\n",
    "print(\"Docs containing 'complain' (stemmed):\")\n",
    "print(msg_index_stem.get('complain', {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 The Problem with Naive Score Summing\n",
    "\n",
    "If we just add scores from multiple fields, we run into problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_multifield_naive(query, indices, num_docs, tokenizer):\n",
    "    \"\"\"Naive multi-field scoring: just sum scores from all fields.\"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    scores = np.zeros(num_docs)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        for field_name, index in indices.items():\n",
    "            df = get_df(token, index)\n",
    "            if df == 0:\n",
    "                continue\n",
    "            idf = 1.0 / df\n",
    "            \n",
    "            for doc_id in range(num_docs):\n",
    "                tf = get_tf(token, doc_id, index)\n",
    "                scores[doc_id] += tf * idf\n",
    "    \n",
    "    return scores\n",
    "\n",
    "indices = {'msg': msg_index_stem, 'name': name_index_stem}\n",
    "scores = score_multifield_naive(\"apostolos complain\", indices, len(msgs), stemming_tokenize)\n",
    "\n",
    "msgs_with_scores = msgs.copy()\n",
    "msgs_with_scores['score'] = scores\n",
    "msgs_with_scores.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: A document matching \"apostolos\" twice (once in name, once in message) scores higher than a document matching both \"apostolos\" AND \"complain\"!\n",
    "\n",
    "## 4.4 Dismax: Term-Centric Search\n",
    "\n",
    "> ðŸ“š **TERM: Dismax (Disjunction Maximum)**  \n",
    "> For each query term, take the MAXIMUM score across all fields instead of summing. This prevents the same term from being counted multiple times.\n",
    "\n",
    "This is called **term-centric** search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_multifield_dismax(query, indices, num_docs, tokenizer):\n",
    "    \"\"\"Dismax multi-field scoring: max score per term across fields.\"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    scores = np.zeros(num_docs)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        # For this term, find the best score across all fields\n",
    "        term_scores = np.zeros(num_docs)\n",
    "        \n",
    "        for field_name, index in indices.items():\n",
    "            df = get_df(token, index)\n",
    "            if df == 0:\n",
    "                continue\n",
    "            idf = 1.0 / df\n",
    "            \n",
    "            for doc_id in range(num_docs):\n",
    "                tf = get_tf(token, doc_id, index)\n",
    "                field_score = tf * idf\n",
    "                term_scores[doc_id] = max(term_scores[doc_id], field_score)\n",
    "        \n",
    "        scores += term_scores  # Add the best score for this term\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = score_multifield_dismax(\"apostolos complain\", indices, len(msgs), stemming_tokenize)\n",
    "\n",
    "msgs_with_scores = msgs.copy()\n",
    "msgs_with_scores['score'] = scores\n",
    "msgs_with_scores.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now documents matching BOTH terms rank higher!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. From TF*IDF to BM25\n",
    "\n",
    "Our simple TF*IDF has two problems that BM25 solves:\n",
    "\n",
    "## 5.1 Problems with Naive TF*IDF\n",
    "\n",
    "### Problem 1: Term Frequency Saturation\n",
    "\n",
    "With raw TF, a document mentioning \"apostolos\" 10 times scores 10x higher than one mentioning it once. But does it really make the document 10x more relevant?\n",
    "\n",
    "This is called **keyword stuffing** â€” repeating words doesn't make a document more relevant after a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: keyword stuffing problem\n",
    "stuffed_docs = [\n",
    "    \"Apostolos has a complaint\",\n",
    "    \"Apostolos Apostolos Apostolos Apostolos Apostolos Apostolos Apostolos Apostolos Apostolos Apostolos loves repetition\"\n",
    "]\n",
    "\n",
    "stuffed_index, stuffed_lengths = build_index(stuffed_docs, better_tokenize)\n",
    "\n",
    "# With raw TF, the stuffed document wins\n",
    "for doc_id, doc in enumerate(stuffed_docs):\n",
    "    tf = get_tf('apostolos', doc_id, stuffed_index)\n",
    "    print(f\"Doc {doc_id}: TF={tf}, Doc: '{doc[:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Field Length Bias\n",
    "\n",
    "A term appearing once in a 5-word tweet is more significant than appearing once in a 5000-word article. The tweet is clearly \"about\" that term; the article just mentions it in passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Fixing Saturation\n",
    "\n",
    "We can use logarithmic saturation to dampen the effect of high term frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log saturation\n",
    "raw_tfs = [1, 2, 5, 10, 20, 50, 100]\n",
    "saturated = [np.log(tf + 1) for tf in raw_tfs]\n",
    "\n",
    "print(\"Raw TF vs Log-Saturated TF:\")\n",
    "for raw, sat in zip(raw_tfs, saturated):\n",
    "    print(f\"  TF={raw:3d} â†’ {sat:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Fixing Length Bias\n",
    "\n",
    "We normalize by document length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length normalization example\n",
    "tf = 1  # Same term frequency\n",
    "short_doc_len = 10\n",
    "long_doc_len = 1000\n",
    "\n",
    "print(f\"TF=1 in {short_doc_len}-word doc: normalized = {tf / short_doc_len:.4f}\")\n",
    "print(f\"TF=1 in {long_doc_len}-word doc: normalized = {tf / long_doc_len:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 BM25: The Formula\n",
    "\n",
    "> ðŸ“š **TERM: BM25 (Best Match 25)**  \n",
    "> A refined scoring formula that addresses TF saturation and length bias. \"25\" refers to it being the 25th iteration of the Best Match formula.\n",
    "\n",
    "The BM25 formula has two key parameters:\n",
    "- **k1** (typically 1.2): Controls how fast TF saturates\n",
    "- **b** (typically 0.75): Controls length normalization (0 = none, 1 = full)\n",
    "\n",
    "```\n",
    "BM25 score = IDF Ã— (TF Ã— (k1 + 1)) / (TF + k1 Ã— (1 - b + b Ã— docLen/avgDocLen))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_idf(df, num_docs):\n",
    "    \"\"\"BM25 IDF formula.\"\"\"\n",
    "    return np.log((num_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "\n",
    "def bm25_tf(tf, doc_len, avg_doc_len, k1=1.2, b=0.75):\n",
    "    \"\"\"BM25 TF normalization.\"\"\"\n",
    "    return (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))\n",
    "\n",
    "def score_bm25(query, index, num_docs, doc_lengths, tokenizer, k1=1.2, b=0.75):\n",
    "    \"\"\"Score documents using BM25.\"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    scores = np.zeros(num_docs)\n",
    "    avg_doc_len = np.mean(doc_lengths)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        df = get_df(token, index)\n",
    "        if df == 0:\n",
    "            continue\n",
    "        \n",
    "        idf = bm25_idf(df, num_docs)\n",
    "        \n",
    "        for doc_id in range(num_docs):\n",
    "            tf = get_tf(token, doc_id, index)\n",
    "            if tf > 0:\n",
    "                tf_norm = bm25_tf(tf, doc_lengths[doc_id], avg_doc_len, k1, b)\n",
    "                scores[doc_id] += idf * tf_norm\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TF*IDF vs BM25 on keyword-stuffed documents\n",
    "stuffed_index, stuffed_lengths = build_index(stuffed_docs, better_tokenize)\n",
    "\n",
    "print(\"Keyword-stuffed documents:\")\n",
    "for i, doc in enumerate(stuffed_docs):\n",
    "    print(f\"  Doc {i}: '{doc}'\")\n",
    "\n",
    "print(\"\\nScoring for query 'apostolos':\")\n",
    "\n",
    "tfidf_scores = score_tfidf(\"apostolos\", stuffed_index, len(stuffed_docs), stuffed_lengths, better_tokenize)\n",
    "bm25_scores = score_bm25(\"apostolos\", stuffed_index, len(stuffed_docs), stuffed_lengths, better_tokenize)\n",
    "\n",
    "print(f\"  TF*IDF: {tfidf_scores}\")\n",
    "print(f\"  BM25:   {bm25_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25 reduces the gap between the stuffed and non-stuffed document!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. BM25F: Multi-Field BM25\n",
    "\n",
    "## 6.1 The Field Blending Problem\n",
    "\n",
    "When we run BM25 on each field separately and then combine scores, we have a problem: IDF is calculated **per field**.\n",
    "\n",
    "A term might be rare in the \"title\" field but common in the \"body\" field. Taking the max gives the title field too much weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo the problem\n",
    "docs_with_topics = [\n",
    "    {\"msg\": \"Hi this is Apostolos, I have a complaint about the weather\", \"topics\": \"weather complaint\"},\n",
    "    {\"msg\": \"Apostolos, we hear you. What's the issue apostolos?\", \"topics\": \"earth climate\"},\n",
    "    {\"msg\": \"Tom, can I speak to your manager?\", \"topics\": \"escalation\"},\n",
    "    {\"msg\": \"I have complaints about skiing\", \"topics\": \"skiing complaint\"},\n",
    "    {\"msg\": \"Thanks you guys are great\", \"topics\": \"gratitude\"},\n",
    "    {\"msg\": \"That's very sweet\", \"topics\": \"apostolos\"}  # \"apostolos\" is rare in topics!\n",
    "]\n",
    "\n",
    "msgs_topics = pd.DataFrame(docs_with_topics)\n",
    "\n",
    "msg_idx, msg_lens = build_index(msgs_topics['msg'].tolist(), better_tokenize)\n",
    "topic_idx, topic_lens = build_index(msgs_topics['topics'].tolist(), better_tokenize)\n",
    "\n",
    "print(f\"'apostolos' DF in msg field: {get_df('apostolos', msg_idx)}\")\n",
    "print(f\"'apostolos' DF in topics field: {get_df('apostolos', topic_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"apostolos\" appears in only 1 document's topics field, making it seem very rare/important there, even though it's common overall.\n",
    "\n",
    "## 6.2 BM25F Solution\n",
    "\n",
    "> ðŸ“š **TERM: BM25F (BM25 for structured documents with Fields)**  \n",
    "> An extension of BM25 that blends term frequencies across fields BEFORE applying IDF. This ensures term rarity is measured across the entire corpus, not per-field.\n",
    "\n",
    "Instead of:\n",
    "```\n",
    "score = BM25(field1) + BM25(field2)  # IDF calculated per field\n",
    "```\n",
    "\n",
    "We do:\n",
    "```\n",
    "combined_tf = TF(field1) + TF(field2)\n",
    "combined_df = max(DF(field1), DF(field2))  # Or sum\n",
    "score = BM25_formula(combined_tf, combined_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bm25f(query, indices, doc_lengths_dict, num_docs, tokenizer, k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25F scoring: blend TF across fields before applying IDF.\n",
    "    \n",
    "    Args:\n",
    "        indices: dict of {field_name: index}\n",
    "        doc_lengths_dict: dict of {field_name: [lengths]}\n",
    "    \"\"\"\n",
    "    query_tokens = tokenizer(query)\n",
    "    scores = np.zeros(num_docs)\n",
    "    \n",
    "    # Calculate average doc length across all fields\n",
    "    total_lengths = np.zeros(num_docs)\n",
    "    for field_lengths in doc_lengths_dict.values():\n",
    "        total_lengths += np.array(field_lengths)\n",
    "    avg_doc_len = np.mean(total_lengths)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        # Calculate combined DF (max across fields)\n",
    "        combined_df = 0\n",
    "        for index in indices.values():\n",
    "            combined_df = max(combined_df, get_df(token, index))\n",
    "        \n",
    "        if combined_df == 0:\n",
    "            continue\n",
    "        \n",
    "        # Use combined DF for IDF\n",
    "        idf = bm25_idf(combined_df, num_docs)\n",
    "        \n",
    "        for doc_id in range(num_docs):\n",
    "            # Combine TF across fields (with length normalization per field)\n",
    "            combined_impact = 0\n",
    "            \n",
    "            for field_name, index in indices.items():\n",
    "                tf = get_tf(token, doc_id, index)\n",
    "                if tf > 0:\n",
    "                    doc_len = doc_lengths_dict[field_name][doc_id]\n",
    "                    avg_field_len = np.mean(doc_lengths_dict[field_name])\n",
    "                    # Length-normalized impact\n",
    "                    impact = tf / (1 - b + b * doc_len / avg_field_len)\n",
    "                    combined_impact += impact\n",
    "            \n",
    "            if combined_impact > 0:\n",
    "                # Apply TF saturation to combined impact\n",
    "                saturated = combined_impact / (combined_impact + k1)\n",
    "                scores[doc_id] += idf * saturated\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare naive multi-field vs BM25F\n",
    "indices = {'msg': msg_idx, 'topics': topic_idx}\n",
    "lengths = {'msg': msg_lens, 'topics': topic_lens}\n",
    "\n",
    "naive_scores = score_multifield_dismax(\"apostolos complaint\", indices, len(msgs_topics), better_tokenize)\n",
    "bm25f_scores = score_bm25f(\"apostolos complaint\", indices, lengths, len(msgs_topics), better_tokenize)\n",
    "\n",
    "comparison = msgs_topics.copy()\n",
    "comparison['naive'] = naive_scores\n",
    "comparison['bm25f'] = bm25f_scores\n",
    "comparison.sort_values('bm25f', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25F gives more balanced results by measuring term rarity across the entire corpus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ¯ Summary\n",
    "\n",
    "## What We Covered\n",
    "\n",
    "| Concept | What It Is | Key Formula/Approach |\n",
    "|---------|-----------|---------------------|\n",
    "| **Tokenization** | Breaking text into searchable terms | lowercase + punctuation removal |\n",
    "| **Inverted Index** | Maps terms to documents | `term â†’ {doc_id: count}` |\n",
    "| **TF** | Term Frequency | Count of term in document |\n",
    "| **IDF** | Inverse Document Frequency | `1 / (docs containing term)` |\n",
    "| **TF*IDF** | Basic relevance scoring | `TF Ã— IDF` |\n",
    "| **BM25** | Improved TF*IDF | Saturation (k1) + length norm (b) |\n",
    "| **BM25F** | Multi-field BM25 | Blend TF before IDF |\n",
    "\n",
    "## âœ… Can You Do These?\n",
    "\n",
    "- [ ] Build a tokenizer that normalizes text\n",
    "- [ ] Create an inverted index from scratch\n",
    "- [ ] Implement OR and AND queries\n",
    "- [ ] Calculate TF, DF, and TF*IDF scores\n",
    "- [ ] Explain why rare terms score higher\n",
    "- [ ] Implement BM25 with k1 and b parameters\n",
    "- [ ] Use dismax for multi-field search\n",
    "- [ ] Implement BM25F field blending\n",
    "\n",
    "## ðŸ†˜ Troubleshooting\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| No search results | Check tokenization â€” are query terms matching index terms? |\n",
    "| Wrong ranking | Review scoring function â€” TF saturation and length norm help |\n",
    "| Multi-field bias | Use dismax or BM25F, not naive sum |\n",
    "| Keyword stuffing dominates | Use BM25's TF saturation (k1 parameter) |\n",
    "\n",
    "## Next Class\n",
    "\n",
    "We'll explore **embedding-based search** and learn when to combine lexical and semantic approaches!\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [BM25 Explained](https://www.elastic.co/blog/practical-bm25-part-1-how-shards-affect-relevance-scoring-in-elasticsearch)\n",
    "- [Elasticsearch Analyzers](https://www.elastic.co/docs/reference/text-analysis/analyzer-reference)\n",
    "- [Introduction to Information Retrieval (Stanford)](https://nlp.stanford.edu/IR-book/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
