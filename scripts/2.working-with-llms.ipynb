{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: Introduction to AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Working with Large Language Models\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Today we will learn how to work with Large Language Models (LLMs) in Python:\n",
    "\n",
    "1. **LiteLLM** - Unified interface for LLM APIs\n",
    "2. **API Keys** - Securely managing credentials\n",
    "3. **Structured Outputs** - Getting reliable, typed responses with Pydantic\n",
    "\n",
    "By the end of this lecture, you'll be able to call any major LLM provider and get structured, validated responses!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. LiteLLM and API Keys\n",
    "\n",
    "## 1.1 What is LiteLLM?\n",
    "\n",
    "**LiteLLM** is a library that provides a unified interface for calling many different LLM providers:\n",
    "\n",
    "| Provider | Example Models |\n",
    "|----------|----------------|\n",
    "| OpenAI | GPT-4, GPT-4o-mini |\n",
    "| Anthropic | Claude 3 Opus, Sonnet, Haiku |\n",
    "| Google | Gemini Pro, Gemini Flash |\n",
    "| And many more... | |\n",
    "\n",
    "### Why Use LiteLLM?\n",
    "\n",
    "- **One API**: Same code works with any provider\n",
    "- **Easy Switching**: Change providers by changing one line\n",
    "- **Fallbacks**: Automatically try another provider if one fails\n",
    "- **Cost Tracking**: Built-in usage and cost monitoring\n",
    "\n",
    "## 1.2 Getting API Keys\n",
    "\n",
    "To use LLMs, you need API keys from providers:\n",
    "\n",
    "### OpenAI\n",
    "1. Go to [platform.openai.com](https://platform.openai.com)\n",
    "2. Sign up / Log in\n",
    "3. Go to API Keys section\n",
    "4. Click \"Create new secret key\"\n",
    "5. Copy and save the key (you can't see it again!)\n",
    "\n",
    "### Anthropic\n",
    "1. Go to [console.anthropic.com](https://console.anthropic.com)\n",
    "2. Sign up / Log in\n",
    "3. Go to API Keys\n",
    "4. Create a new key\n",
    "\n",
    "### Google (Gemini)\n",
    "1. Go to [aistudio.google.com](https://aistudio.google.com)\n",
    "2. Sign in with Google\n",
    "3. Click \"Get API key\"\n",
    "4. Create a key for a new or existing project\n",
    "\n",
    "## 1.3 Storing API Keys Securely\n",
    "\n",
    "**Never put API keys directly in your code!** Instead, use a `.env` file:\n",
    "\n",
    "### Step 1: Create a `.env` file in your project root\n",
    "\n",
    "```bash\n",
    "# In your terminal\n",
    "touch .env\n",
    "```\n",
    "\n",
    "### Step 2: Add your keys to the `.env` file\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-your-openai-key-here\n",
    "ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "### Step 3: Add `.env` to `.gitignore`\n",
    "\n",
    "```bash\n",
    "echo \".env\" >> .gitignore\n",
    "```\n",
    "\n",
    "This prevents your keys from being uploaded to GitHub!\n",
    "\n",
    "## 1.4 Loading API Keys in Python\n",
    "\n",
    "Use the `python-dotenv` package to load your keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm and python-dotenv are already installed via uv sync\n",
    "import litellm\n",
    "import warnings\n",
    "\n",
    "# Suppress Pydantic serialization warnings from LiteLLM\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pydantic serializer warnings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if keys are loaded (don't print the actual keys!)\n",
    "print(\"OpenAI key loaded:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"Anthropic key loaded:\", \"ANTHROPIC_API_KEY\" in os.environ)\n",
    "print(\"Google key loaded:\", \"GOOGLE_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.5 Using LiteLLM\n",
    "\n",
    "Here's how to call different LLM providers with the same code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function to call any LLM\n",
    "def ask_llm(question: str, model: str = \"gpt-4o-mini\") -> str:\n",
    "    \"\"\"\n",
    "    Ask a question to an LLM.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "        model: The model to use (e.g., \"gpt-4o-mini\", \"claude-3-haiku-20240307\", \"gemini/gemini-pro\")\n",
    "    \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Function defined! Now you can use ask_llm() to query any LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ask OpenAI's GPT-4o-mini\n",
    "response = ask_llm(\"What is Python in one sentence?\", model=\"gpt-4o-mini\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ask Anthropic's Claude\n",
    "response = ask_llm(\"What is Python in one sentence?\", model=\"claude-3-haiku-20240307\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 1.6 Model Names Reference\n",
    "\n",
    "Here are common models you can use with LiteLLM:\n",
    "\n",
    "| Provider | Model Name | Notes |\n",
    "|----------|-----------|-------|\n",
    "| **OpenAI** | `gpt-4o` | Most capable |\n",
    "| | `gpt-4o-mini` | Fast and cheap |\n",
    "| | `gpt-4-turbo` | Good balance |\n",
    "| **Anthropic** | `claude-3-opus-20240229` | Most capable |\n",
    "| | `claude-3-sonnet-20240229` | Good balance |\n",
    "| | `claude-3-haiku-20240307` | Fast and cheap |\n",
    "| **Google** | `gemini/gemini-pro` | General purpose |\n",
    "| | `gemini/gemini-pro-vision` | With images |\n",
    "\n",
    "## 1.7 Best Practices\n",
    "\n",
    "1. **Never commit API keys** - Always use `.env` files\n",
    "2. **Start with cheap models** - Use `gpt-4o-mini` or `claude-3-haiku` for testing\n",
    "3. **Handle errors** - APIs can fail; wrap calls in try/except\n",
    "4. **Monitor usage** - Keep track of your API costs\n",
    "5. **Use fallbacks** - LiteLLM can automatically try another provider if one fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Structured Outputs with Pydantic\n",
    "\n",
    "When we call LLMs, we usually get back free-form text. But often, we want **structured data** â€” like JSON objects with specific fields and types.\n",
    "\n",
    "**Why structured outputs?**\n",
    "- **Reliability**: Guaranteed format makes parsing easier\n",
    "- **Type safety**: We know exactly what fields exist and their types\n",
    "- **Validation**: Invalid responses are caught automatically\n",
    "- **Integration**: Easy to use with databases, APIs, and other systems\n",
    "\n",
    "**The problem with unstructured outputs:**\n",
    "\n",
    "If we ask an LLM to extract information from a movie review, we might get:\n",
    "- \"The review is positive. The rating is 4.5 stars.\"\n",
    "- \"Rating: 4.5/5, Sentiment: positive\"\n",
    "- \"Positive review, 4.5 stars\"\n",
    "\n",
    "Each format is different, making it hard to parse consistently.\n",
    "\n",
    "**The solution: Pydantic**\n",
    "\n",
    "Pydantic is a Python library that lets us define data models with types and validation. OpenAI's API can return responses that match these models exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 2.1 Defining a Pydantic Model\n",
    "\n",
    "First, we define what structure we want using a Pydantic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Define the structure we want\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"Information extracted from a movie review\"\"\"\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field(\n",
    "        description=\"The overall sentiment of the review\"\n",
    "    )\n",
    "    rating: float = Field(\n",
    "        description=\"Numeric rating from 1.0 to 5.0\",\n",
    "        ge=1.0,\n",
    "        le=5.0\n",
    "    )\n",
    "    key_points: list[str] = Field(\n",
    "        description=\"List of main points mentioned in the review\",\n",
    "        min_length=1,\n",
    "        max_length=5\n",
    "    )\n",
    "    reviewer_name: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Name of the reviewer if mentioned\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 2.2 Using Structured Outputs with OpenAI\n",
    "\n",
    "Now we can ask the LLM to return data in this exact format using `response_format`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "review_text = \"\"\"\n",
    "This movie was absolutely fantastic! The cinematography was stunning, \n",
    "and the acting performances were top-notch. I'd give it 4.5 stars. \n",
    "The plot kept me engaged from start to finish. Highly recommend!\n",
    "- Sarah Johnson\n",
    "\"\"\"\n",
    "\n",
    "# Request structured output using LiteLLM\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Extract information from movie reviews. Return structured data.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Extract information from this review:\\n\\n{review_text}\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=MovieReview  # LiteLLM supports Pydantic models directly!\n",
    ")\n",
    "\n",
    "# Parse the response into our Pydantic model\n",
    "import json\n",
    "review_data = MovieReview.model_validate_json(response.choices[0].message.content)\n",
    "\n",
    "print(f\"Sentiment: {review_data.sentiment}\")\n",
    "print(f\"Rating: {review_data.rating}\")\n",
    "print(f\"Key Points: {review_data.key_points}\")\n",
    "print(f\"Reviewer: {review_data.reviewer_name}\")\n",
    "print(f\"\\nAs JSON:\\n{review_data.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 2.3 Best Practices for Structured Outputs\n",
    "\n",
    "**Key benefits:**\n",
    "- **Type safety**: Your IDE can autocomplete fields and catch errors\n",
    "- **Validation**: Pydantic automatically validates the data matches your schema\n",
    "- **Documentation**: The model serves as documentation of what data you expect\n",
    "\n",
    "**Best practices:**\n",
    "- Use `Field()` to add descriptions â€” these help the LLM understand what to extract\n",
    "- Use `Literal` types for constrained choices (like sentiment categories)\n",
    "- Make optional fields explicit with `| None`\n",
    "- Add validation constraints (like `ge`, `le` for numeric ranges)\n",
    "\n",
    "**When to use:**\n",
    "- Extracting structured data from unstructured text\n",
    "- Building APIs that need consistent response formats\n",
    "- Data processing pipelines where you need reliable parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ¯ Summary\n",
    "\n",
    "Today we covered how to work with LLMs in Python:\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|-------------|\n",
    "| **LiteLLM** | Unified API for all LLM providers |\n",
    "| **API Keys** | Store in `.env`, never commit to git |\n",
    "| **Pydantic** | Define schemas for structured LLM outputs |\n",
    "\n",
    "## Key Code Patterns\n",
    "\n",
    "```python\n",
    "# Basic LLM call\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "\n",
    "# Structured output\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[...],\n",
    "    response_format=MyPydanticModel\n",
    ")\n",
    "```\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [LiteLLM Docs](https://docs.litellm.ai)\n",
    "- [Pydantic Docs](https://docs.pydantic.dev/)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs)\n",
    "- [Anthropic API Reference](https://docs.anthropic.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ce)",
   "language": "python",
   "name": "ce"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
