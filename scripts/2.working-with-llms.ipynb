{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Working with Large Language Models (LLMs) via API\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Today we'll learn how to work with Large Language Models (LLMs) through their APIs. By the end of this lecture, you'll be able to:\n",
    "\n",
    "1. **Use Provider SDKs** - Call OpenAI, Anthropic, and Google directly\n",
    "2. **Use LiteLLM** - One unified interface for all providers\n",
    "3. **Handle Failures** - Implement retries with exponential backoff\n",
    "4. **Understand Pydantic** - Python's data validation library\n",
    "5. **Get Structured Outputs** - Guaranteed JSON responses from LLMs\n",
    "6. **Use Async** - Make concurrent API calls for speed\n",
    "7. **Use Instructor** - Alternative approach to structured outputs\n",
    "8. **Generate Images** - Create images with Google Imagen\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Setup: API Keys and Environment\n",
    "\n",
    "Before we can call any LLM, we need API keys. These are like passwords that identify you to the service.\n",
    "\n",
    "## 1.1 Getting API Keys\n",
    "\n",
    "| Provider | Where to Get Key | Cost |\n",
    "|----------|-----------------|------|\n",
    "| **OpenAI** | [platform.openai.com](https://platform.openai.com) | Pay-as-you-go |\n",
    "| **Anthropic** | [console.anthropic.com](https://console.anthropic.com) | Pay-as-you-go |\n",
    "| **Google** | [aistudio.google.com](https://aistudio.google.com) | Free tier available |\n",
    "\n",
    "## 1.2 Storing Keys Securely\n",
    "\n",
    "Never put API keys in your code! Instead, use a `.env` file (in your project root)\n",
    "```python\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "ANTHROPIC_API_KEY=sk-ant-your-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "Make sure `.env` is in your `.gitignore` so you don't accidentally commit your keys!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress noisy warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify keys are loaded (don't print actual keys!)\n",
    "print(\"OpenAI key loaded:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"Anthropic key loaded:\", \"ANTHROPIC_API_KEY\" in os.environ)\n",
    "print(\"Google key loaded:\", \"GOOGLE_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Provider SDKs: The Native Way\n",
    "\n",
    "Each LLM provider has their own Python SDK. Let's see how each one works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 OpenAI SDK\n",
    "\n",
    "OpenAI's SDK is the most widely used. It powers the GPT-4.1 series and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create a client (automatically uses OPENAI_API_KEY from environment)\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Make a chat completion request\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is Python in exactly one sentence?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "print(\"OpenAI Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Response Structure\n",
    "\n",
    "The response is a `ChatCompletion` object. Key parts:\n",
    "\n",
    "```python\n",
    "response.choices[0].message.content  # The actual text response\n",
    "response.choices[0].finish_reason    # Why generation stopped (\"stop\", \"length\", etc.)\n",
    "response.usage.total_tokens          # Tokens used (for billing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Anthropic SDK\n",
    "\n",
    "Anthropic makes Claude models. Their API is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "# Create a client\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "# Make a message request (note: different method name!)\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=1024,  # Required for Anthropic!\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is Python in exactly one sentence?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the response (different structure!)\n",
    "print(\"Anthropic Response:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic Response Structure\n",
    "\n",
    "Different from OpenAI:\n",
    "\n",
    "```python\n",
    "response.content[0].text    # The actual text (not .message.content!)\n",
    "response.stop_reason        # Why it stopped\n",
    "response.usage.input_tokens # Tokens used\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Google Gemini SDK\n",
    "\n",
    "Google's SDK is different again. They use the `google-genai` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# Create a client\n",
    "google_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Generate content (yet another API style!)\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    contents=\"What is Python in exactly one sentence?\"\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "print(\"Google Response:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The Problem: Three APIs, Three Formats\n",
    "\n",
    "Notice how each provider has:\n",
    "- Different client initialization\n",
    "- Different method names (`chat.completions.create` vs `messages.create` vs `generate_content`)\n",
    "- Different response structures\n",
    "- Different parameter names\n",
    "\n",
    "This makes it hard to:\n",
    "- Switch providers\n",
    "- Compare models\n",
    "- Write reusable code\n",
    "\n",
    "**Solution: LiteLLM** - One API to rule them all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. LiteLLM: Unified Interface\n",
    "\n",
    "LiteLLM provides a single, consistent API that works with 100+ LLM providers.\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **One API** | Same code for any provider |\n",
    "| **Easy Switching** | Change one line to switch models |\n",
    "| **Fallbacks** | Automatically try another provider if one fails |\n",
    "| **Cost Tracking** | Built-in usage monitoring |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Same function works with ANY provider!\n",
    "def ask_llm(prompt: str, model: str = \"gpt-5-mini\") -> str:\n",
    "    \"\"\"Ask any LLM a question using LiteLLM.\"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "print(\"GPT-5-mini:\", ask_llm(\"Say 'hello' in French\", model=\"gpt-5-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic (same code, different model!)\n",
    "print(\"Claude Haiku:\", ask_llm(\"Say 'hello' in French\", model=\"claude-3-5-haiku-20241022\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google (same code, just add \"gemini/\" prefix)\n",
    "print(\"Gemini Flash:\", ask_llm(\"Say 'hello' in French\", model=\"gemini/gemini-2.5-flash\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Name Reference\n",
    "\n",
    "| Provider | Model Name | Notes |\n",
    "|----------|-----------|-------|\n",
    "| **OpenAI** | `gpt-5.2` | Most capable (latest) |\n",
    "| | `gpt-5.2-pro` | More compute, better answers |\n",
    "| | `gpt-5.1` | Great for coding/agentic tasks |\n",
    "| | `gpt-5-mini` | Fast and cheap |\n",
    "| **Anthropic** | `claude-opus-4-5-20251101` | Most capable |\n",
    "| | `claude-sonnet-4-5-20250929` | Best balance |\n",
    "| | `claude-3-5-haiku-20241022` | Fast and cheap |\n",
    "| **Google** | `gemini/gemini-2.5-pro` | Most capable |\n",
    "| | `gemini/gemini-3.0-flash` | Fast and cheap |\n",
    "\n",
    "**Note**: We use `gpt-5-mini` in examples because it's cost-effective for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Messages and Conversation History\n",
    "\n",
    "LLMs understand different \"roles\" in a conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        # System message: sets the behavior/personality\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful pirate. Always respond like a pirate.\"},\n",
    "        # User message: the actual question\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Making API Calls Robust: Retries\n",
    "\n",
    "API calls can fail for many reasons:\n",
    "- **Rate limiting** (HTTP 429) - too many requests\n",
    "- **Server errors** (HTTP 500) - provider issues\n",
    "- **Timeouts** - network issues\n",
    "\n",
    "**Solution:** Retry with exponential backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Manual Retry Pattern\n",
    "\n",
    "Here's how to implement retries from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def call_with_retries(prompt: str, max_retries: int = 5, base_delay: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Call LLM with exponential backoff retry.\n",
    "    \n",
    "    Wait times: 1s -> 2s -> 4s -> 8s -> 16s (plus random jitter)\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = litellm.completion(\n",
    "                model=\"gpt-5-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                raise  # Give up after max retries\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            delay = base_delay * (2 ** (attempt - 1)) + random.random()\n",
    "            print(f\"Attempt {attempt} failed: {e}. Retrying in {delay:.1f}s...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "# Test it\n",
    "result = call_with_retries(\"Say 'hello'\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using Tenacity Library\n",
    "\n",
    "Writing retry logic is tedious. The `tenacity` library makes it elegant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),           # Max 5 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=60),  # Exponential backoff\n",
    "    reraise=True                          # Re-raise the exception if all retries fail\n",
    ")\n",
    "def robust_llm_call(prompt: str, model: str = \"gpt-5-mini\") -> str:\n",
    "    \"\"\"Call LLM with automatic retries.\"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# The decorator handles all retry logic!\n",
    "result = robust_llm_call(\"What is 2 + 2?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Built-in Client Retries\n",
    "\n",
    "The OpenAI client has built-in retry support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create client with automatic retries\n",
    "client = OpenAI(\n",
    "    max_retries=5,  # Automatically retry up to 5 times\n",
    "    timeout=30.0    # Timeout after 30 seconds\n",
    ")\n",
    "\n",
    "# Now all calls through this client will automatically retry!\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. What is Pydantic?\n",
    "\n",
    "**Pydantic** is Python's most popular data validation library. It lets you:\n",
    "\n",
    "- Define data structures with types\n",
    "- Automatically validate data\n",
    "- Get helpful error messages\n",
    "- Serialize to/from JSON\n",
    "\n",
    "It's used everywhere in modern Python: FastAPI, LangChain, Django Ninja, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Basic Pydantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a data structure\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int = Field(ge=0, le=150)  # Must be 0-150\n",
    "    email: str | None = None        # Optional field\n",
    "\n",
    "# Create an instance - Pydantic validates automatically!\n",
    "person = Person(name=\"Alice\", age=30, email=\"alice@example.com\")\n",
    "print(person)\n",
    "print(f\"Name: {person.name}, Age: {person.age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic catches invalid data\n",
    "try:\n",
    "    invalid_person = Person(name=\"Bob\", age=200)  # Age > 150!\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic auto-converts types when possible\n",
    "person = Person(name=\"Charlie\", age=\"25\")  # String \"25\" -> int 25\n",
    "print(f\"Age is {person.age}, type: {type(person.age)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Serialization (to/from JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON\n",
    "person = Person(name=\"Diana\", age=28)\n",
    "json_str = person.model_dump_json(indent=2)\n",
    "print(\"As JSON:\")\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse from JSON\n",
    "json_data = '{\"name\": \"Eve\", \"age\": 35, \"email\": \"eve@example.com\"}'\n",
    "person = Person.model_validate_json(json_data)\n",
    "print(f\"Parsed: {person.name}, {person.age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Why Pydantic Matters for LLMs\n",
    "\n",
    "LLMs return free-form text. Pydantic lets us:\n",
    "\n",
    "1. **Define what we want** - Create a schema/model\n",
    "2. **Get structured data** - LLM returns JSON matching our schema\n",
    "3. **Validate automatically** - Pydantic checks the data is correct\n",
    "4. **Use easily** - Access fields with dot notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Structured Outputs with Pydantic\n",
    "\n",
    "The **problem**: LLMs return free-form text that's hard to parse.\n",
    "\n",
    "Ask \"extract info from this review\" and you might get:\n",
    "- \"The sentiment is positive and the rating is 4.5\"\n",
    "- \"Rating: 4.5/5, Sentiment: positive\"\n",
    "- \"Positive review! 4.5 stars.\"\n",
    "\n",
    "**Solution**: Use `response_format` to get guaranteed JSON structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Define the structure we want\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"Structured data extracted from a movie review.\"\"\"\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field(\n",
    "        description=\"The overall sentiment of the review\"\n",
    "    )\n",
    "    rating: float = Field(\n",
    "        description=\"Numeric rating from 1.0 to 5.0\",\n",
    "        ge=1.0,\n",
    "        le=5.0\n",
    "    )\n",
    "    key_points: list[str] = Field(\n",
    "        description=\"Main points mentioned in the review (1-5 items)\",\n",
    "        min_length=1,\n",
    "        max_length=5\n",
    "    )\n",
    "    reviewer_name: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Name of the reviewer if mentioned\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample review to analyze\n",
    "review_text = \"\"\"\n",
    "This movie was absolutely fantastic! The cinematography was stunning, \n",
    "and the acting performances were top-notch. I'd give it 4.5 stars. \n",
    "The plot kept me engaged from start to finish. Highly recommend!\n",
    "- Sarah Johnson\n",
    "\"\"\"\n",
    "\n",
    "# Use LiteLLM with response_format\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Extract structured information from movie reviews.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Extract information from this review:\\n\\n{review_text}\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=MovieReview  # Tell the LLM to return this structure!\n",
    ")\n",
    "\n",
    "# Parse the JSON response into our Pydantic model\n",
    "review_data = MovieReview.model_validate_json(response.choices[0].message.content)\n",
    "\n",
    "# Now we have clean, typed data!\n",
    "print(f\"Sentiment: {review_data.sentiment}\")\n",
    "print(f\"Rating: {review_data.rating}\")\n",
    "print(f\"Key Points: {review_data.key_points}\")\n",
    "print(f\"Reviewer: {review_data.reviewer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Nested Models\n",
    "\n",
    "You can create complex structures with nested Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(BaseModel):\n",
    "    \"\"\"Information about an actor.\"\"\"\n",
    "    name: str\n",
    "    role: str\n",
    "\n",
    "class MovieInfo(BaseModel):\n",
    "    \"\"\"Comprehensive movie information.\"\"\"\n",
    "    title: str\n",
    "    year: int = Field(ge=1900, le=2030)\n",
    "    genre: list[str]\n",
    "    director: str\n",
    "    actors: list[Actor]  # Nested model!\n",
    "    plot_summary: str = Field(max_length=500)\n",
    "\n",
    "# Use it\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me information about the movie Inception\"}\n",
    "    ],\n",
    "    response_format=MovieInfo\n",
    ")\n",
    "\n",
    "movie = MovieInfo.model_validate_json(response.choices[0].message.content)\n",
    "print(f\"Title: {movie.title} ({movie.year})\")\n",
    "print(f\"Director: {movie.director}\")\n",
    "print(f\"Genres: {', '.join(movie.genre)}\")\n",
    "print(f\"\\nActors:\")\n",
    "for actor in movie.actors:\n",
    "    print(f\"  - {actor.name} as {actor.role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Async Programming\n",
    "\n",
    "**Problem**: If you need to make 100 LLM calls, doing them one-by-one is slow.\n",
    "\n",
    "**Solution**: Make them concurrently with async programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 What is Async?\n",
    "\n",
    "Think of it like ordering at a restaurant:\n",
    "\n",
    "- **Synchronous**: Order one dish, wait for it, eat it, then order the next\n",
    "- **Asynchronous**: Order all dishes at once, they arrive as they're ready\n",
    "\n",
    "Key Python concepts:\n",
    "\n",
    "| Keyword | Meaning |\n",
    "|---------|---------|\n",
    "| `async def` | Defines an async function |\n",
    "| `await` | Pause here until the result is ready |\n",
    "| `asyncio.gather()` | Run multiple async tasks concurrently |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Sequential vs Concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# A list of prompts to process\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the capital of Germany?\",\n",
    "    \"What is the capital of Italy?\",\n",
    "    \"What is the capital of Spain?\",\n",
    "    \"What is the capital of Portugal?\",\n",
    "    \"What is the capital of Greece?\",\n",
    "    \"What is the capital of Turkey?\",\n",
    "    \"What is the capital of Bulgaria?\",\n",
    "    \"What is the capital of Romania?\",\n",
    "    \"What is the capital of Hungary?\",\n",
    "    \"What is the capital of Poland?\",\n",
    "    \"What is the capital of Czech Republic?\",\n",
    "]\n",
    "\n",
    "# Sequential: one at a time\n",
    "start = time.time()\n",
    "sequential_results = []\n",
    "for prompt in prompts:\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    sequential_results.append(response.choices[0].message.content)\n",
    "sequential_time = time.time() - start\n",
    "\n",
    "print(f\"Sequential: {sequential_time:.2f} seconds\")\n",
    "for prompt, result in zip(prompts, sequential_results):\n",
    "    print(f\"  {prompt} -> {result[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Async function to call LLM\n",
    "async def async_ask(prompt: str) -> str:\n",
    "    \"\"\"Make an async LLM call.\"\"\"\n",
    "    response = await litellm.acompletion(  # Note: acompletion, not completion!\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Async function to process all prompts concurrently\n",
    "async def process_all(prompts: list[str]) -> list[str]:\n",
    "    \"\"\"Process all prompts concurrently.\"\"\"\n",
    "    tasks = [async_ask(prompt) for prompt in prompts]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Run concurrently\n",
    "start = time.time()\n",
    "concurrent_results = await process_all(prompts)\n",
    "concurrent_time = time.time() - start\n",
    "\n",
    "print(f\"Concurrent: {concurrent_time:.2f} seconds\")\n",
    "print(f\"Speedup: {sequential_time / concurrent_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 When to Use Async\n",
    "\n",
    "Use async when:\n",
    "- Processing many items (batch analysis)\n",
    "- Making independent API calls\n",
    "- Building web applications (FastAPI uses async)\n",
    "\n",
    "Don't bother for:\n",
    "- Single API calls\n",
    "- Sequential workflows where each step depends on the previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Instructor Library\n",
    "\n",
    "**Instructor** is another approach to structured outputs. It:\n",
    "\n",
    "- Patches existing clients (OpenAI, Anthropic, etc.)\n",
    "- Returns Pydantic objects directly (not JSON strings)\n",
    "- Has built-in retry and validation\n",
    "- Works with multiple providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Patch the OpenAI client with Instructor\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "# Define what we want\n",
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    occupation: str\n",
    "\n",
    "# Use it - returns a Pydantic object directly!\n",
    "user = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Generate a random person's info\"}\n",
    "    ],\n",
    "    response_model=UserInfo  # Instructor uses response_model, not response_format\n",
    ")\n",
    "\n",
    "# user is already a UserInfo object!\n",
    "print(f\"Name: {user.name}\")\n",
    "print(f\"Age: {user.age}\")\n",
    "print(f\"Occupation: {user.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Instructor with Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "# Patch Anthropic client\n",
    "anthropic_instructor = instructor.from_anthropic(Anthropic())\n",
    "\n",
    "# Same interface!\n",
    "user = anthropic_instructor.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Generate a random person's info\"}\n",
    "    ],\n",
    "    response_model=UserInfo\n",
    ")\n",
    "\n",
    "print(f\"From Claude: {user.name}, {user.age}, {user.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Instructor vs LiteLLM: When to Use Each\n",
    "\n",
    "| Feature | LiteLLM | Instructor |\n",
    "|---------|---------|------------|\n",
    "| **Multi-provider** | Yes (100+ providers) | Yes (OpenAI, Anthropic, etc.) |\n",
    "| **Structured output** | Returns JSON string | Returns Pydantic object |\n",
    "| **Retry built-in** | No (use tenacity) | Yes |\n",
    "| **Validation retry** | No | Yes (retries if validation fails) |\n",
    "| **Learning curve** | Lower | Slightly higher |\n",
    "\n",
    "**Use LiteLLM when**: You want unified access to many providers\n",
    "\n",
    "**Use Instructor when**: You want the cleanest structured output experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Image Generation with Gemini (Nano Banana)\n",
    "\n",
    "Gemini models can generate images natively - no separate Imagen model needed! This feature is called \"Nano Banana\" internally at Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from pathlib import Path\n",
    "\n",
    "# Create client\n",
    "google_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Generate an image using Nano Banana (Gemini's native image generation)\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",  # Nano Banana model\n",
    "    contents=[\"A cute robot learning to code, digital art style\"],\n",
    ")\n",
    "\n",
    "# Find and save the image from the response\n",
    "output_path = Path(\"temp/robot_coding.png\")\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "for part in response.parts:\n",
    "    if part.inline_data is not None:\n",
    "        image = part.as_image()\n",
    "        image.save(output_path)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image in Jupyter\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if output_path.exists():\n",
    "    display(Image(filename=str(output_path), width=400))\n",
    "else:\n",
    "    print(\"Run the previous cell first to generate the image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Nano Banana Pro (Gemini 3)\n",
    "\n",
    "For complex prompts that need reasoning, use **Nano Banana Pro**. It \"thinks\" before generating, making it great for:\n",
    "- Infographics with text\n",
    "- Complex compositions\n",
    "- Multi-element scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nano Banana Pro - uses \"thinking\" for complex prompts\n",
    "from google.genai import types\n",
    "\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-3-pro-image-preview\",  # Nano Banana Pro\n",
    "    contents=[\"Create a detailed infographic showing how neural networks learn, \"\n",
    "              \"with clear labels, arrows showing data flow, and a modern tech aesthetic\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['TEXT', 'IMAGE'],\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            include_thoughts=True  # Enable \"thinking\" mode\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the pro image\n",
    "pro_output_path = Path(\"temp/neural_network_infographic.png\")\n",
    "\n",
    "for part in response.parts:\n",
    "    if part.inline_data is not None:\n",
    "        image = part.as_image()\n",
    "        image.save(pro_output_path)\n",
    "        print(f\"Pro image saved to {pro_output_path}\")\n",
    "        break\n",
    "    elif part.text is not None:\n",
    "        print(f\"Model's thoughts: {part.text[:200]}...\")  # Show reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Pro image\n",
    "if pro_output_path.exists():\n",
    "    display(Image(filename=str(pro_output_path), width=500))\n",
    "else:\n",
    "    print(\"Run the previous cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Tips for Image Generation\n",
    "\n",
    "**Nano Banana models**:\n",
    "| Model | ID | Best For |\n",
    "|-------|-----|----------|\n",
    "| **Nano Banana** | `gemini-2.5-flash-image` | Fast, high-volume tasks |\n",
    "| **Nano Banana Pro** | `gemini-3-pro-image-preview` | Pro quality, complex prompts |\n",
    "\n",
    "**Good prompts**:\n",
    "- Be specific: \"A golden retriever puppy playing in autumn leaves, photograph\"\n",
    "- Include style: \"digital art\", \"oil painting\", \"photograph\", \"3D render\"\n",
    "- Describe lighting: \"sunset lighting\", \"studio lighting\"\n",
    "\n",
    "**With aspect ratio**:\n",
    "```python\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"your prompt\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['TEXT', 'IMAGE'],\n",
    "        image_config=types.ImageConfig(aspect_ratio=\"16:9\")\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "**Edit existing images**:\n",
    "```python\n",
    "from PIL import Image\n",
    "img = Image.open(\"photo.png\")\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"Add a wizard hat\", img],  # Pass image + edit instruction\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: All images include an invisible SynthID watermark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10. Summary\n",
    "\n",
    "Today we covered a lot! Here's a quick reference:\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|-------------|\n",
    "| **Provider SDKs** | Each provider has different APIs (OpenAI, Anthropic, Google) |\n",
    "| **LiteLLM** | Unified interface for all providers - use this! |\n",
    "| **Retries** | Use `tenacity` or built-in client retries for robustness |\n",
    "| **Pydantic** | Data validation library for defining schemas |\n",
    "| **Structured Outputs** | Use `response_format` for guaranteed JSON |\n",
    "| **Async** | Use `acompletion` + `asyncio.gather` for concurrent calls |\n",
    "| **Instructor** | Alternative for structured outputs with validation retries |\n",
    "| **Nano Banana** | Gemini's native image generation (`gemini-2.5-flash-image`) |\n",
    "\n",
    "## Quick Code Reference\n",
    "\n",
    "```python\n",
    "# Basic LLM call with LiteLLM\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "\n",
    "# Structured output\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[...],\n",
    "    response_format=MyPydanticModel\n",
    ")\n",
    "data = MyPydanticModel.model_validate_json(response.choices[0].message.content)\n",
    "\n",
    "# Async call\n",
    "response = await litellm.acompletion(model=\"gpt-5-mini\", messages=[...])\n",
    "\n",
    "# Image generation (Nano Banana)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"your prompt\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [LiteLLM Documentation](https://docs.litellm.ai)\n",
    "- [Pydantic Documentation](https://docs.pydantic.dev)\n",
    "- [Instructor Documentation](https://python.useinstructor.com)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs)\n",
    "- [Anthropic API Reference](https://docs.anthropic.com)\n",
    "- [Google Gemini API](https://ai.google.dev/gemini-api/docs)\n",
    "- [Tenacity Documentation](https://tenacity.readthedocs.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ce)",
   "language": "python",
   "name": "ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
